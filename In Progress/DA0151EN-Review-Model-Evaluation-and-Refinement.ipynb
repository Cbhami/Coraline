{"cells":[{"cell_type":"markdown","metadata":{},"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDA0151ENSkillsNetwork21582452-2022-01-01\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n","    </a>\n","</p>\n","\n","<h1>Model Evaluation and Refinement with Tidymodels</h1>\n","\n","Estimated Time Needed: **45 min**\n"]},{"cell_type":"markdown","metadata":{},"source":["### Welcome!\n","\n","We have built models and made predictions of flight delays. Now we will determine how accurate these predictions are.\n","\n","You will learn techniques for evaluating the performance of your models. This inludes how to split your dataset into training and testing sets, build and train linear regression models with a training set, compute metrics to assess the performance of models, and tune hyperparameters. Moreover, you‚Äôll also learn a technique for handling cases with small datasets.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Table of Contents\n","\n","*   [1. Model Evaluation](#evaluation)\n","*   [2. Over-fitting, Under-fitting and Model Selection](#selection)\n","*   [3. Regularization: Ridge regression, Lasso regression and Elastic Net](#ridge)\n","*   [4. Grid Search](#grid)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Setup\n","\n","<p>\n","Before you can start diving into model evaluation, you first need to load the libraries and data.\n"]},{"cell_type":"markdown","metadata":{},"source":["<h4>Load Libraries</h4> \n","\n","In the previous labs, you used base R's `lm()` to create linear regression models. In this lab, we will introduce another way to create models with **Tidymodels**.\n","\n","Tidymodels is a collection of packages that use tidyverse principles to easily do the entire modeling process from preprocessing initial data, to creating a model, to tunning hyperparameters.\n","\n","Below, install \"tidymodels\", additionally \"rlang\" should be updated in order to properly run \"tidymodels\".\n","\n","**Note**: The installation of \"rlang\" and \"tidymodels\" may take a while in the Skill Network Labs. Please give it some time or rerun the cell if you are waiting for more than 10 minutes.\n"]},{"cell_type":"code","execution_count":4,"metadata":{"vscode":{"languageId":"r"}},"outputs":[{"name":"stderr","output_type":"stream","text":["Installing package into 'C:/Users/coleb/OneDrive/Documents/R/win-library/4.1'\n","(as 'lib' is unspecified)\n","\n"]},{"name":"stdout","output_type":"stream","text":["package 'rlang' successfully unpacked and MD5 sums checked\n"]},{"name":"stderr","output_type":"stream","text":["Warning message:\n","\"cannot remove prior installation of package 'rlang'\"\n","Warning message in file.copy(savedcopy, lib, recursive = TRUE):\n","\"problem copying C:\\Users\\coleb\\OneDrive\\Documents\\R\\win-library\\4.1\\00LOCK\\rlang\\libs\\x64\\rlang.dll to C:\\Users\\coleb\\OneDrive\\Documents\\R\\win-library\\4.1\\rlang\\libs\\x64\\rlang.dll: Permission denied\"\n","Warning message:\n","\"restored 'rlang'\"\n"]},{"name":"stdout","output_type":"stream","text":["\n","The downloaded binary packages are in\n","\tC:\\Users\\coleb\\AppData\\Local\\Temp\\RtmpUzJbKt\\downloaded_packages\n"]},{"name":"stderr","output_type":"stream","text":["Installing package into 'C:/Users/coleb/OneDrive/Documents/R/win-library/4.1'\n","(as 'lib' is unspecified)\n","\n","also installing the dependencies 'dplyr', 'tibble'\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["package 'dplyr' successfully unpacked and MD5 sums checked\n"]},{"name":"stderr","output_type":"stream","text":["Warning message:\n","\"cannot remove prior installation of package 'dplyr'\"\n","Warning message in file.copy(savedcopy, lib, recursive = TRUE):\n","\"problem copying C:\\Users\\coleb\\OneDrive\\Documents\\R\\win-library\\4.1\\00LOCK\\dplyr\\libs\\x64\\dplyr.dll to C:\\Users\\coleb\\OneDrive\\Documents\\R\\win-library\\4.1\\dplyr\\libs\\x64\\dplyr.dll: Permission denied\"\n","Warning message:\n","\"restored 'dplyr'\"\n"]},{"name":"stdout","output_type":"stream","text":["package 'tibble' successfully unpacked and MD5 sums checked\n"]},{"name":"stderr","output_type":"stream","text":["Warning message:\n","\"cannot remove prior installation of package 'tibble'\"\n","Warning message in file.copy(savedcopy, lib, recursive = TRUE):\n","\"problem copying C:\\Users\\coleb\\OneDrive\\Documents\\R\\win-library\\4.1\\00LOCK\\tibble\\libs\\x64\\tibble.dll to C:\\Users\\coleb\\OneDrive\\Documents\\R\\win-library\\4.1\\tibble\\libs\\x64\\tibble.dll: Permission denied\"\n","Warning message:\n","\"restored 'tibble'\"\n"]},{"name":"stdout","output_type":"stream","text":["package 'tidymodels' successfully unpacked and MD5 sums checked\n","\n","The downloaded binary packages are in\n","\tC:\\Users\\coleb\\AppData\\Local\\Temp\\RtmpUzJbKt\\downloaded_packages\n"]}],"source":["# Install tidymodels if you haven't done so\n","install.packages(\"rlang\")\n","install.packages(\"tidymodels\")"]},{"cell_type":"markdown","metadata":{},"source":["After installing the packages, load them. As with the other labs, we will be using \"tidyverse\" as well.\n"]},{"cell_type":"code","execution_count":5,"metadata":{"vscode":{"languageId":"r"}},"outputs":[{"ename":"ERROR","evalue":"Error: package or namespace load failed for 'tidymodels' in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):\n namespace 'rlang' 1.0.2 is already loaded, but >= 1.0.3 is required\n","output_type":"error","traceback":["Error: package or namespace load failed for 'tidymodels' in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):\n namespace 'rlang' 1.0.2 is already loaded, but >= 1.0.3 is required\nTraceback:\n","1. library(tidymodels)","2. tryCatch({\n .     attr(package, \"LibPath\") <- which.lib.loc\n .     ns <- loadNamespace(package, lib.loc)\n .     env <- attachNamespace(ns, pos = pos, deps, exclude, include.only)\n . }, error = function(e) {\n .     P <- if (!is.null(cc <- conditionCall(e))) \n .         paste(\" in\", deparse(cc)[1L])\n .     else \"\"\n .     msg <- gettextf(\"package or namespace load failed for %s%s:\\n %s\", \n .         sQuote(package), P, conditionMessage(e))\n .     if (logical.return && !quietly) \n .         message(paste(\"Error:\", msg), domain = NA)\n .     else stop(msg, call. = FALSE, domain = NA)\n . })","3. tryCatchList(expr, classes, parentenv, handlers)","4. tryCatchOne(expr, names, parentenv, handlers[[1L]])","5. value[[3L]](cond)","6. stop(msg, call. = FALSE, domain = NA)"]}],"source":["# Library for modeling\n","library(tidymodels)\n","\n","# Load tidyverse\n","library(tidyverse)"]},{"cell_type":"markdown","metadata":{},"source":["#### Load Data\n"]},{"cell_type":"markdown","metadata":{},"source":["As a reminder, you can find the \"Airline Data Set\" from the following link: <a href=\"https://dax-cdn.cdn.appdomain.cloud/dax-airline/1.0.1/lax_to_jfk.tar.gz?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDA0151ENSkillsNetwork21582452-2022-01-01\"><https://dax-cdn.cdn.appdomain.cloud/dax-airline/1.0.1/lax_to_jfk.tar.gz></a>.\n","We will be using the LAX to JFK sample data set throughout this course.\n","\n","</p>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["# url where the data is located\n","url <- \"https://dax-cdn.cdn.appdomain.cloud/dax-airline/1.0.1/lax_to_jfk.tar.gz\"\n","\n","# download the file\n","download.file(url, destfile = \"lax_to_jfk.tar.gz\")\n","\n","# untar the file so we can get the csv only\n","# if you run this on your local machine, then can remove tar = \"internal\" \n","untar(\"lax_to_jfk.tar.gz\", tar = \"internal\")\n","\n","# read_csv only \n","sub_airline <- read_csv(\"lax_to_jfk/lax_to_jfk.csv\",\n","                     col_types = cols('DivDistance' = col_number(), \n","                                      'DivArrDelay' = col_number()))"]},{"cell_type":"markdown","metadata":{},"source":["<a class=\"anchor\" id=\"evaluation\"></a>\n","\n","## 1. Model Evaluation\n","\n","### 1.1 Training and Testing Data\n","\n","An important step in testing your model is to split your data into training and testing data. The training data will be used to train (fit) models, while the testing data will not be touched until we are evaluating the model.\n","\n","Using other packages or programming languages may require to separate out the reponse variable (`ArrDelayMinutes` in this case) into another dataframe, but here that is not necessary. The response and predictor variables can all stay in one dataframe.\n","\n","Before splitting the data we:\n","\n","*   Use the principles learned in module 2 and use `replace_na()` to replace the NAs in the variables we are using to predict. Here, we choose to replace the values with 0 because having NA in these variables mean that there was no delay.\n","*   Use `select()` to only include the variables we will use to create a final model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["flight_delays <- sub_airline %>% \n","    replace_na(list(CarrierDelay = 0,\n","                    WeatherDelay = 0,\n","                    NASDelay = 0,\n","                    SecurityDelay = 0,\n","                    LateAircraftDelay = 0)) %>%\n","    select(c(ArrDelayMinutes, DepDelayMinutes, CarrierDelay, WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay, DayOfWeek, Month))"]},{"cell_type":"markdown","metadata":{},"source":["Now, with the prepared dataset `flight_delays`, you can split the data. A random seed is set so that the way the data is split will be the same every time this code is run, this helps create reproducible results.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["set.seed(1234)\n","flight_split <- initial_split(flight_delays)\n","train_data <- training(flight_split)\n","test_data <- testing(flight_split)"]},{"cell_type":"markdown","metadata":{},"source":["In `initial_split()`, you can also set the `prop` parameter to set the proportion of the data to use for training. If it is unspecified like here in the example, then by default it is set to 0.75. This means that the proportion of data that is split into the training data is 75% (so the testing data is 25%).\n"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n","<h1> Question  #1):</h1>\n","\n","<b>Use the function \"initial_split\" to split up the data set such that 80% of the data samples will be utilized for training. The output of the function should be the following:  \"flight_split2\", \"train_data2\" , \"test_data2\".</b>\n","\n","</div>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for the solution.</summary>\n","\n","```r\n","flight_split2 <- initial_split(flight_delays, prop = 4/5)  # prop = 0.8 works as well\n","train_data2 <- training(flight_split2)\n","test_data2 <- testing(flight_split2)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2 Training a Model\n","\n","After splitting the dataset, the next step is to create a Linear Regression object by using `linear_reg()` to specify linear regression and `set_engine()` to specify which package is used to create the model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["# Pick linear regression\n","lm_spec <- linear_reg() %>%\n","  # Set engine\n","  set_engine(engine = \"lm\")\n","\n","# Print the linear function\n","lm_spec"]},{"cell_type":"markdown","metadata":{},"source":["In this example, we will use Arrival Delay Minutes (\"ArrDelayMinutes\") as the response variable and Departure Delay Minutes (\"DepDelayMinutes\") as the predictor variable to fit (train) a model. We will use `train_data` because we are training the model. The `test_data` will be used later.\n","\n","Use `fit()` to fit the model we just specified in `lm_spec`. The output is the fitted (trained) model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["train_fit <- lm_spec %>% \n","    fit(ArrDelayMinutes ~ DepDelayMinutes, data = train_data)\n","\n","train_fit "]},{"cell_type":"markdown","metadata":{},"source":["To look at some of the predictions of the fitted model, use `predict()`, which will output one column with predictions (`.pred`). Here, since `new_data = train_data`, you are looking at how well the model is predicting the original training data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["train_results <- train_fit %>%\n","  # Make the predictions and save the predicted values\n","  predict(new_data = train_data) %>%\n","  # Create a new column to save the true values\n","  mutate(truth = train_data$ArrDelayMinutes)\n","\n","head(train_results)"]},{"cell_type":"markdown","metadata":{},"source":["Additionally, you can use the same fitted model to predict on test data and save to a dataset called `test_results`. There are two columns in the dataset, including both predicted values and true values.\n","\n","Now it is time to evaluate the models to estimate how well the models perform on new data, the test data. This example uses the same model in `train_fit` to make the predictions. Again, from `predict()`, the output is stored in a data frame with only one column, called `.pred`. You can then add a new column to this data frame using the `mutate()` function. This new column is named `truth` and contains values of \"ArrDelayMinutes\" from the `test_data`. In the end, you will have a dataframe with the predictions and the true values.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["test_results <- train_fit %>%\n","  # Make the predictions and save the predicted values\n","  predict(new_data = test_data) %>%\n","  # Create a new column to save the true values\n","  mutate(truth = test_data$ArrDelayMinutes)\n","\n","head(test_results)"]},{"cell_type":"markdown","metadata":{},"source":["### 1.3 Evaluating the Model\n","\n","Next, let's evaluate the model. Using metrics learned in previous lessons like RMSE or R$^2$ are good ways to evaluate *regression* models.\n","\n","In previous lessons you learned how to claculate RMSE with combinations of functions like `mean()` and `sqrt()`, which is a good exercise. However in practice, this may not be ideal. So more conveniently with \"tidymodels\", there are already functions like `rmse()` as well as many other metric functions (see [https://yardstick.tidymodels.org/reference/index.html](https://yardstick.tidymodels.org/reference/index.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDA0151ENSkillsNetwork21582452-2022-01-01)).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["rmse(train_results, truth = truth,\n","     estimate = .pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["rmse(test_results, truth = truth,\n","     estimate = .pred)"]},{"cell_type":"markdown","metadata":{},"source":["Using `rsq()`, let's lalculate the R-squared on the training and test data:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["rsq(train_results, truth = truth,\n","    estimate = .pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["rsq(test_results, truth = truth,\n","    estimate = .pred)"]},{"cell_type":"markdown","metadata":{},"source":["You can also make a plot to visualize how well you predicted¬†the Arrival Delay Minutes.\n","\n","This example plots the actual values (the true values of ArrDelayMinutes) versus the model predictions for both the testing and training datasets. It also plots the line y = x through the origin. This line is a visual representation of the perfect model where all predicted values are equal to the true values in the test set. The farther the points are from this line, the worse the model fit.\n","\n","Let's break down the code below:\n","\n","1.  `mutate` - add column called `train` to test_results and set the values all to \"testing\"\n","2.  `bind_rows` - do the same to the train_results and bind these rows the test_results\n","3.  `ggplot` - plot the truth vs prediction values\n","4.  `geom_abline` - add the y=x line\n","5.  `geom_point` - add the truth vs prediction points to the plot\n","6.  `facet_wrap` - since `train` contains two values \"testing\" and \"training\", this splits the data into two graphs\n","7.  `labs` - add labels\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["test_results %>%\n","  mutate(train = \"testing\") %>%\n","  bind_rows(train_results %>% mutate(train = \"training\")) %>%\n","  ggplot(aes(truth, .pred)) +\n","  geom_abline(lty = 2, color = \"orange\", \n","              size = 1.5) +\n","  geom_point(color = '#006EA1', \n","             alpha = 0.5) +\n","  facet_wrap(~train) +\n","  labs(x = \"Truth\", \n","       y = \"Predicted Arrival Delays (min)\")"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n","<h1> Question  #2): </h1>\n","<p> \n","Using \"ArrDelayMinutes\" as the response variable and \"DepDelayMinutes\" as the predictor variable, find the R^2  on the test data using 80% of the data for training data.\n","</p>\n","<p>Hint: use train_data2 from question 1. </p>\n","</div>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for the solution.</summary>\n","\n","```r\n","train_fit2 <- lm_spec %>% \n","    fit(ArrDelayMinutes ~ DepDelayMinutes, \n","    data = train_data2)\n","test_results2 <- train_fit2 %>%\n","  # Make the predictions and save the predicted values\n","  predict(new_data = test_data2) %>%\n","  # Create a new column to save the true values\n","  mutate(truth = test_data2$ArrDelayMinutes)\n","rsq(test_results2, truth = truth,\n","    estimate = .pred)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["Sometimes you may not have a large enough testing data; as a result, you may want to perform cross validation. Let's  go over several methods that you can use for  cross validation.\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.4 Cross validation\n"]},{"cell_type":"markdown","metadata":{},"source":["One of the most common ‚Äúout-of-sample‚Äù evaluation techniques is **cross validation**.\n","\n","Cross validation is an effective use of data because each observation is used for both training and testing. In cross validation,\n","\n","1.  First, the dataset is split into k-equal groups; each group is referred to as a fold.\n","2.  k - 1 of the folds are used to train a model, and the remaining fold is used to test with an evaluation metric.\n","3.  This is **repeated** until each of the k groups is used as the test set.\n","4.  After all folds are used, there are k evaluation metric results. They are **averaged** to get an estimate of out-of-sample error\n","\n","For example, in 4-fold cross validation you would use three folds for training and then use one fold for testing. The same model would be trained and then tested 4 times using an evaluation metric. The evaluation metric that you use depends on the model, we will use RMSE and R-squared in our code example.\n","\n","#### Why is it worth the effort to perform cross validation?\n","\n","Using cross validation means that a model is trained and evaluated many (k) times, however it is still worth the computational cost because it is used to test the generalizability of the model. Generalizability is a measure of how useful the results of a study are for a broader group of people and situations. As you train a model on the training set, it tends to overfit most of the time. To avoid this situation, you can use regularization techniques. Cross validation provides a check on how the model is performing on a test data (new unseen data), and since you have limited training instances, you need to be careful while reducing the amount of training samples and reserving it for testing purposes.\n","\n","Moreover, cross validation still works well with a **small amount of data**. For example, assume that you only have 100 samples. If you do a train test with an 80 to 20 percent split, then you only have 20 samples in the test set, which is too small to generalize reliable results. With cross validation, you can have as many as k-folds, so you can build k different models. In this case, you can make predictions on all your data and then average out the model performance.\n","\n","#### Code Example\n","\n","To perform cross validation, you can use `vfold_cv()`. Setting `v = 10` means that it will use 10 folds. The function `fit_resamples()` will keep refitting the model specified on the samples specified by the cross validation object.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["set.seed(1234)\n","cv_folds <- vfold_cv(train_data, v = 10)\n","results <- fit_resamples(lm_spec, \n","                         ArrDelayMinutes ~ DepDelayMinutes,\n","                         resamples = cv_folds)"]},{"cell_type":"markdown","metadata":{},"source":["We can calculate the **average** RMSE and R-squared of our estimate:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["results %>% collect_metrics()"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n","<h1> Question  #3): </h1>\n","<b> \n","Calculate the average RMSE and R-squared using three folds utilizing DepDelayMinutes as a feature : \n","</b>\n","</div>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for the solution.</summary>\n","\n","```r\n","cv_folds_3 <- vfold_cv(train_data, v = 3)\n","results <- fit_resamples(\n","    lm_spec, \n","    ArrDelayMinutes ~ DepDelayMinutes, \n","    resamples = cv_folds_3)\n","results %>% collect_metrics()\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["<a class=\"anchor\" id=\"selection\"></a>\n","\n","## 2. Overfitting, Underfitting and Model Selection\n","\n","It turns out that the test data sometimes referred to as the out of sample data is a much better measure of how well your model performs in the real world. One reason is **underfitting**. A model that is underfit will have high training and high testing error.\n","\n","#### How to prevent underfitting?\n","\n","*   Increase the model complexity\n","*   Add more features to the training data\n","*   Try different models\n","\n","Let's go over an example of underfitting using a simple dataset included with R called \"cars\". We will predict the distance (`dist`) it takes for cars to stop using the car's speed (`speed`).\n","\n","In this first example model, the model is defined a line set to the mean of the car's stopping distance. Based on the plot, this model is underfitting because of the speeds less than 10 and greater than 20 are very far from the prediction line.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[{"ename":"ERROR","evalue":"Error in ggplot(cars, aes(x = speed, y = dist)): could not find function \"ggplot\"\n","output_type":"error","traceback":["Error in ggplot(cars, aes(x = speed, y = dist)): could not find function \"ggplot\"\nTraceback:\n"]}],"source":["ggplot(cars, aes(x = speed, y = dist)) + \n","    geom_point() + \n","    geom_hline(yintercept = mean(cars$dist), \n","               col = \"red\") "]},{"cell_type":"markdown","metadata":{},"source":["Another reason that using the test data to measure the performance of the model is because of **overfitting**. These differences are more apparent in Multiple Linear Regression and Polynomial Regression so we will explore overfitting in that context.\n","\n","#### How to prevent overfitting?\n","\n","*   Reduce model complexity\n","*   Training with more data\n","*   Cross-validation\n","*   Regularization\n","\n","Let's take a look at an example. We use 8th degree polynomial here with `poly(x, 8)` to fit the \"car\" dataset.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["ggplot(cars, aes(x = speed, y = dist)) + \n","    geom_point() + \n","    geom_smooth(method = \"lm\", \n","                formula = y ~ poly(x, 8), \n","                col = \"red\", se = FALSE) "]},{"cell_type":"markdown","metadata":{},"source":["The model is fitting to the points in the top right. If this model received new speeds, it may not be able to predict accurate distances.\n"]},{"cell_type":"markdown","metadata":{},"source":["Going back to the example with the \"cars\" dataset, you can reduce the complexity of the model. In the previous overfitting example, a polynomial model of 8 degrees was used. Instead, you can use a polynomial of degree 1 or a simple linear regression model. In R, you can set the formula to y over x. In this example, we demonstrated how you can prevent overfitting and underfitting models by changing the model complexity.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["ggplot(cars, aes(x = speed, y = dist)) + \n","    geom_point() + \n","    geom_smooth(method = \"lm\", \n","                formula = y ~ x, \n","                col = \"red\", \n","                se = FALSE) "]},{"cell_type":"markdown","metadata":{},"source":["<a class=\"anchor\" id=\"ridge\"></a>\n","\n","## 3. Regularization\n","\n","Regularization is a way to handle the problem of overfitting. It is a technique you can use to reduce the complexity of the model by adding a penalty on the different parameters of the model. After it is applied, the model will be¬†less likely to fit the noise¬†of the training data and will improve the generalization abilities of the model. So, regularization is a way of *avoiding overfitting* by restricting the magnitude of model coefficients.\n","\n","There are a few methods of regularizing linear models including\n","\n","*   Ridge (L2) regularization\n","*   Lasso (L1) regularization\n","*   Elastic net (mix of L1 and L2) regularization\n"]},{"cell_type":"markdown","metadata":{},"source":["### Ridge (L2) regularization\n","\n","First, create a `recipe()` that includes the model formula. You could preprocess the data more in this step, but the data here is already preprocessed. The dot `.` in the formula is a special character that tells R to use all the variables in train_data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["flight_recipe <-\n","  recipe(ArrDelayMinutes ~ ., data = train_data)"]},{"cell_type":"markdown","metadata":{},"source":["Next, use the `linear_reg()` function from the tidymodels library to specify the model.\n","\n","‚Äúpenalty‚Äù is the value of lambda. ‚Äùmixture‚Äù is the proportion of L1 penalty. For ridge regression, specify **`mixture = 0`**. This means there is no L1 penalty and only the L2 penalty is used. For lasso regression, you would use **`mixture = 1`**.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["ridge_spec <- linear_reg(penalty = 0.1, mixture = 0) %>%\n","  set_engine(\"glmnet\")"]},{"cell_type":"markdown","metadata":{},"source":["Next, create a workflow object so you can more conveniently combine pre-processing, modeling, and post-processing requests.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["ridge_wf <- workflow() %>%\n","  add_recipe(flight_recipe)"]},{"cell_type":"markdown","metadata":{},"source":["Finally, add the ridge model and fit the model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["ridge_fit <- ridge_wf %>%\n","  add_model(ridge_spec) %>%\n","  fit(data = train_data)"]},{"cell_type":"markdown","metadata":{},"source":["To view the result of the fitted ridge regression model, use the `pull_workflow_fit()` function.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["ridge_fit %>%\n","  pull_workflow_fit() %>%\n","  tidy()"]},{"cell_type":"markdown","metadata":{},"source":["There are two results columns. The estimate column contains the estimates of the coefficients learned by the model. Penalty contains the value of lambda, which in this example is 0.1.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Lasso (L1) regularization\n","\n","Similarly, here is the code for lasso regression.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["lasso_spec <- linear_reg(penalty = 0.1, mixture = 1) %>%\n","  set_engine(\"glmnet\")\n","\n","lasso_wf <- workflow() %>%\n","  add_recipe(flight_recipe)\n","\n","lasso_fit <- lasso_wf %>%\n","  add_model(lasso_spec) %>%\n","  fit(data = train_data)"]},{"cell_type":"markdown","metadata":{},"source":["### Elastic Net (L1 and L2) Regularization\n","\n","Moreover, here is the code for elastic net regularization. Like mentioned before, `mixture` is the proportion of L1 penalty used. Since elastic net uses a combination of L1 and L2 regularization, then when `mixture` is set to a value between 0 and 1 (not including 0 and 1) then it is considered elastic net regularization. In this example, it uses less L1 penalty than L2.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["elasticnet_spec <- linear_reg(penalty = 0.1, mixture = 0.3) %>%\n","  set_engine(\"glmnet\")\n","\n","elasticnet_wf <- workflow() %>%\n","  add_recipe(flight_recipe)\n","  \n","elasticnet_fit <- elasticnet_wf %>%\n","  add_model(elasticnet_spec) %>%\n","  fit(data = train_data)"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n","<h1> Question  #4): </h1>\n","\n","Perform elastic net regression with \"mixture = 0.5\" and \"penalty = 0.2\" using all features (variables) in the training data, and then output the result of the fitted regression model.\n","\n","</div>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for the solution.</summary>\n","\n","```r\n","flight_recipe <-\n","  recipe(ArrDelayMinutes ~ ., data = train_data)\n","\n","el_spec <- linear_reg(penalty = 0.5, mixture = 0.2) %>%\n","  set_engine(\"glmnet\")\n","\n","el_wf <- workflow() %>%\n","  add_recipe(flight_recipe)\n","\n","el_fit <- el_wf %>%\n","  add_model(el_spec) %>%\n","  fit(data = train_data)\n","\n","el_fit %>%\n","  pull_workflow_fit() %>%\n","  tidy()\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["### Comparing Regularization Types\n","\n","Now that you know more about regularization, it is also good to understand when you would use a techinque over the other.\n","\n","*   **Lasso (L1)**:\n","    *   Pros: Lasso is primarily used for variable selection, that is, reducing the number of variables/features used in a model by shrinking the coefficients to zero. You would use this if you have many variables and think just a select few would will be useful in a final model.\n","    *   Cons: The downside of Lasso is that its variable selection is unstable, as in, for correlated variables it will arbitrarily select one. Additionally, if the number of data point (n) is less than the number of features (p), then Lasso can select at most n of the features.\n","\n","*   **Ridge (L2)**:\n","    *   Pros: If you don‚Äôt want to reduce the number of variables, you can use this. Ridge also works well when there is multicollinearity in the features because it reduces the variance while increasing bias.\n","    *   Cons: Will not reduce the number of variables if that is your goal. Also, the bias in the model may be high.\n","\n","*   **Elastic net (L1/L2)**:\n","    *   Pros: Elastic net combines the benefits of Lasso and Ridge. It solves some of the issues that Lasso has when doing variable selection because it works well when the variables are highly correlated and it can work when the number of variables is greater than the number of samples.\n","    *   Cons: May be computationally more expensive than Lasso or Ridge because it computes both L1 and L2 penalties.\n"]},{"cell_type":"markdown","metadata":{},"source":["<a class=\"anchor\" id=\"grid\"></a>\n","\n","## Part 4: Grid Search\n","\n","The goal of grid search is to find the values of the hyperparameters that results in the best model. This is known as tuning hyperparameters. Hyperparameters are parameters that are not derived from training the model. For example: ùúÜ (or lambda) in ridge/lasso is a hyperparameter.\n","\n","Grid search takes a list of values for each hyperparameter it is tuning and iterates through each combination. It then uses every combination of parameters to produce a model. For each model, a metric like RMSE is calculated. You then determine the best value of the hyperparameters by choosing the model with the best RMSE. In R, you can use functions in tidymodels to run grid search.\n"]},{"cell_type":"markdown","metadata":{},"source":["First, define the lasso model. In this example, we will be tuning a lasso model so `mixture = 1`. We will tune lambda, which is `penalty` in the function.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["tune_spec <- linear_reg(penalty = tune(), mixture = 1) %>% \n","  set_engine(\"glmnet\")\n","\n","lasso_wf <- workflow() %>%\n","  add_recipe(flight_recipe)"]},{"cell_type":"markdown","metadata":{},"source":["Next, define cross validation to resample the data:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["flight_cvfolds <- vfold_cv(train_data)"]},{"cell_type":"markdown","metadata":{},"source":["Now, you can set up the grid using `grid_regular()`. The `levels` are how many values to use and in `penalty()` you can specify the range of values to use. By default, the range values are inverse log transformed. This means that $-3$ is really $10^{-3}$ and $0.3$ is really $10^{0.3}$.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["lambda_grid <- grid_regular(levels = 50,\n","  penalty(range = c(-3, 0.3)))"]},{"cell_type":"markdown","metadata":{},"source":["To tune the grid, use `tune_grid()` and include the lambda grid just specified.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["lasso_grid <- tune_grid(\n","    lasso_wf %>% add_model(tune_spec), \n","    resamples = flight_cvfolds, \n","    grid = lambda_grid)"]},{"cell_type":"markdown","metadata":{},"source":["Finally, to view best results:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["show_best(lasso_grid, metric = \"rmse\")"]},{"cell_type":"markdown","metadata":{},"source":["From the table and using RMSE as the metric, using lambda (penalty) equal to 1.46 gives the best result.\n","\n","Additionally, to visualize the RMSE results:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["lasso_grid %>%\n","  collect_metrics() %>%\n","  filter(.metric == \"rmse\") %>%\n","  ggplot(aes(penalty, mean)) +\n","  geom_line(size=1, color=\"red\") +\n","  scale_x_log10() +\n","  ggtitle(\"RMSE\")"]},{"cell_type":"markdown","metadata":{},"source":["The dip in the RMSE graph corresponds to the best value for lambda. So again, we see that using lambda (penalty) of about 1.46 gives the best result.\n"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n","<h1> Question  #5): </h1>\n","Perform a grid search for the lambda (penalty) parameter on ridge regression, then find the best values of the parameter.\n","</div>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for the solution.</summary>\n","\n","```r\n","tune_spec <- linear_reg(\n","             penalty = tune(), \n","             mixture = 0) %>% \n","  set_engine(\"glmnet\")\n","\n","ridge_grid <- tune_grid(ridge_wf %>% \n","    add_model(tune_spec), \n","    resamples = flight_cvfolds, \n","    grid = lambda_grid)\n","\n","show_best(ridge_grid, metric = \"rmse\")\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["<h1>Thank you for completing this notebook!</h1>\n","\n","Checkout the documentation here: [https://www.tidymodels.org/](https://www.tidymodels.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDA0151ENSkillsNetwork21582452-2022-01-01).\n"]},{"cell_type":"markdown","metadata":{},"source":["<h3>About the Authors:</h3>\n","\n","This notebook was written by <a href=\"https://www.linkedin.com/in/yiwen-li-47a019119/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDA0151ENSkillsNetwork21582452-2022-01-01\" target=\"_blank\">Yiwen Li</a> and <a href=\"https://www.linkedin.com/in/gabrieladequeiroz/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDA0151ENSkillsNetwork21582452-2022-01-01\" target=\"_blank\">Gabriela de Queiroz</a>.\n","\n","<p><a href=\"https://www.linkedin.com/in/yiwen-li-47a019119/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDA0151ENSkillsNetwork21582452-2022-01-01\" target=\"_blank\">Yiwen Li</a> has approximately three year experiences in big tech industry. Currently, she is a developer advocate, a data scientist, a product manager at IBM, where she designs and develops data science solutions and Machine Learning models to solve real world problems. She has delivered talks this year in JupyterCon, PyCon, Pyjamas, CrowdCast.ai, Global AI on Tour 2020 and Belpy 2021 with hundreds of attendants per talk. \n","\n","<a href=\"https://www.linkedin.com/in/gabrieladequeiroz/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDA0151ENSkillsNetwork21582452-2022-01-01\" target=\"_blank\">Gabriela de Queiroz</a> is a Sr. Engineering & Data Science Manager at IBM where she manages and leads a team of developers working on Data & AI Open Source projects. She works to democratize AI by building tools and launching new open source projects.\n","She is the founder of AI Inclusive, a global organization that is helping increase the representation and participation of gender minorities in Artificial Intelligence. She is also the founder of R-Ladies, a worldwide organization for promoting diversity in the R community with more than 190 chapters in 50+ countries.\n","She has worked in several startups and where she built teams, developed statistical models, and employed a variety of techniques to derive insights and drive data-centric decisions\n"]},{"cell_type":"markdown","metadata":{},"source":["<hr>\n","<p>Copyright &copy; 2021 IBM Corporation. All rights reserved.</p>\n"]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"codemirror_mode":"r","file_extension":".r","mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"4.1.3"}},"nbformat":4,"nbformat_minor":4}
