{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOvfHMS4Aq9j",
    "tags": []
   },
   "source": [
    "# MapMyRun Data\n",
    "\n",
    "Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import seaborn as sns \n",
    "sns.set()\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using AWS SQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peewee in c:\\users\\coleb\\mambaforge\\lib\\site-packages (3.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install sshtunnel\n",
    "%pip install peewee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import peewee\n",
    "from peewee import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pymyssql (from versions: none)\n",
      "ERROR: No matching distribution found for pymyssql\n"
     ]
    }
   ],
   "source": [
    "%pip install pymyssql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql.cursors\n",
    "import os\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "import pymysql\n",
    "import django\n",
    "#import MySQLdb\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Environment Variables to connect to the database\n",
    "host = os.getenv('AWShost')\n",
    "user = os.getenv('AWSUN')\n",
    "passwd = os.getenv('AWSPW')\n",
    "db = os.getenv('AWSdb')\n",
    "port = os.getenv('AWSport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if 'RDS_HOSTNAME' in os.environ:\n",
    "    DATABASES = {\n",
    "        'default': {\n",
    "            'ENGINE': 'django.db.backends.mysql',\n",
    "            'NAME': os.environ['AWSdb'],\n",
    "            'USER': os.environ['AWSUN'],\n",
    "            'PASSWORD': os.environ['AWSPW'],\n",
    "            'HOST': os.environ['AWShost'],\n",
    "            'PORT': os.environ['AWSport'],\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Install pymysql package\n",
    "# %pip install pymysql\n",
    "\n",
    "# #Import pymysql package to do database programming with Python. Here we set the values for varaiables\n",
    "# #database_instance_endpoint = the db endpoint from AWS RDS Interface\n",
    "# #port = 3306 which is the port for MySQL\n",
    "# #user = the Master Username defined in AWS RDS DB\n",
    "# #password = password defined when creating DB\n",
    "# import pymysql\n",
    "# database_instance_endpoint=host\n",
    "# port=port\n",
    "# dbname=db\n",
    "# user=user\n",
    "# password=passwd\n",
    "\n",
    "# # Now we will connect to the AWS RDS Database using the command pymysql.connect with the Database details from above.\n",
    "# # Then we store this value in the variable \"connection\"\n",
    "# connection = pymysql.connect(database_instance_endpoint,\n",
    "#                       user = user,\n",
    "#                       port = port,\n",
    "#                       passwd = password,\n",
    "#                       database = dbname)\n",
    "\n",
    "# # Declare varaiable for mycur which is a cursor. We need a cursor to query the database\n",
    "# # Using cursor, we can connect to the database\n",
    "# mycur = connection.cursor()\n",
    "\n",
    "# # Using MySQL query to create a new table called \"students\" with columns: id, firstname, lastname, grade with id as PRIMARY KEY\n",
    "# # Store the MySQL command in a variable \"create_table_query\"\n",
    "# create_table_query=\"\"\"CREATE TABLE IF NOT EXISTS `students` (\n",
    "#                     `id` int(11) NOT NULL AUTO_INCREMENT,\n",
    "#                     `firstname` varchar(255) NOT NULL,`lastname` varchar(255) NOT NULL,`grade` varchar(10),\n",
    "#                      PRIMARY KEY (`id`)\n",
    "#                      ) ENGINE=INNODB;\"\"\"\n",
    "\n",
    "# # Using cursor to execute the command to create the table\t\t\t\t\n",
    "# mycur.execute(create_table_query)\n",
    "\n",
    "# # Using MySQL command to insert a data row into the students table.\n",
    "# # Keey this MySQL command in the variable \"insert_query\"\n",
    "# insert_query=\"INSERT INTO `students` (`id`, `firstname`, `lastname`) VALUES (%s, %s, %s)\"\n",
    "\n",
    "# # Using cursor to execute the insert query command to add more data into the table\n",
    "# mycur.execute(insert_query, ('12345', 'Tata', 'Tutu'))\n",
    "# mycur.execute(insert_query, ('34567', 'Momo', 'Meme'))\n",
    "\n",
    "# # Run the commit command to commit the change into the database\n",
    "# connection.commit()\n",
    "\n",
    "# # Check the result by query all the data from the students table\n",
    "# mycur.execute(\"SELECT * FROM students\")\n",
    "\n",
    "# # Run the command to display the database table\n",
    "# mycur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pymysql\n",
    "\n",
    "# connection = pymysql.connect(host=host,user=user,password=passwd,database=db,charset='utf8mb4',cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "\n",
    "# cursor = connection.cursor()\n",
    "# cursor.execute(\"SELECT VERSION()\")\n",
    "# data = cursor.fetchone()\n",
    "# print (\"Database version : %s \" % data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pymysql\n",
    "\n",
    "# connection = pymysql.connect(host=host,user=user,password=passwd,database=db,charset='utf8mb4',cursorclass=pymysql.cursors.DictCursor, port = 3306)\n",
    "\n",
    "\n",
    "# cursor = connection.cursor()\n",
    "# cursor.execute(\"SELECT VERSION()\")\n",
    "# data = cursor.fetchone()\n",
    "# print (\"Database version : %s \" % data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mysql.connector\n",
    "# import sys\n",
    "# import boto3\n",
    "# import os\n",
    "\n",
    "# ENDPOINT= host\n",
    "# PORT=\"3306\"\n",
    "# USER=user\n",
    "# REGION=\"us-east-2\"\n",
    "# DBNAME=db\n",
    "# os.environ['LIBMYSQL_ENABLE_CLEARTEXT_PLUGIN'] = '1'\n",
    "\n",
    "# #gets the credentials from .aws/credentials\n",
    "# session = boto3.Session(profile_name='default')\n",
    "# client = session.client('rds')\n",
    "\n",
    "# token = client.generate_db_auth_token(DBHostname=ENDPOINT, Port=PORT, DBUsername=USER, Region=REGION)\n",
    "\n",
    "# try:\n",
    "#     conn =  mysql.connector.connect(host=ENDPOINT, user=USER, passwd=token, port=PORT, database=DBNAME, ssl_ca='SSLCERTIFICATE')\n",
    "#     cur = conn.cursor()\n",
    "#     cur.execute(\"\"\"SELECT now()\"\"\")\n",
    "#     query_results = cur.fetchall()\n",
    "#     print(query_results)\n",
    "# except Exception as e:\n",
    "#     print(\"Database connection failed due to {}\".format(e))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(passwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def start_rds_connection():\n",
    "#     try:\n",
    "#         connection = pymysql.connect(host=host,\n",
    "#                                      port=port,\n",
    "#                                      user=user,\n",
    "#                                      passwd=passwd,\n",
    "#                                      db=db,\n",
    "#                                      cursorclass=CURSORCLASS,\n",
    "#                                      ssl_ca=SSL_CA)\n",
    "#         print('[+] RDS Connection Successful')\n",
    "#     except Exception as e:\n",
    "#         print(f'[-] RDS Connection Failed: {e}')\n",
    "#         connection = None\n",
    "\n",
    "#     return connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyodbc \n",
    "# cnxn = pyodbc.connect('DRIVER={MySQL ODBC 8.0 Driver};User ID=admin;Password=GOymra11!!;Server=database-2.c4eszdapvtut.us-east-2.rds.amazonaws.com;Port=3306;String Types=Unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !/usr/bin/env python\n",
    "# # -*- coding: utf-8 -*-\n",
    "# import pymysql\n",
    "\n",
    "# host = host\n",
    "# user = user\n",
    "# password = passwd\n",
    "# database = db\n",
    "\n",
    "# connection = pymysql.connect(host, user, password, database)\n",
    "# with connection:\n",
    "#     cur = connection.cursor()\n",
    "#     cur.execute(\"SELECT VERSION()\")\n",
    "#     version = cur.fetchone()\n",
    "#     print(\"Database version: {} \".format(version[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SSH (ec2_public_dns, ec2_user, pem_path, remote_bind_address=(rds_instance_access_point, port))\n",
    "# with SSHTunnelForwarder(('database-2.c4eszdapvtut.us-east-2.rds.amazonaws.com'), ssh_username=\"admin\", ssh_pkey=passwd, remote_bind_address=('database-2.c4eszdapvtut.us-east-2.rds.amazonaws.com', 3306)) as tunnel:\n",
    "#     print(\"****SSH Tunnel Established****\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mydb = pymysql.connect(\n",
    "#     host=host,\n",
    "#     user=user,\n",
    "#     passwd=passwd,\n",
    "#     database=\"database-2\",\n",
    "#     port=3306\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Oracle DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install cx-Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cx_Oracle\n",
    "\n",
    "# connection = cx_Oracle.connect('cbhami02/GOymra11!!@localhost:33060/database-2')\n",
    "# cursor = connection.cursor()\n",
    "# #connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install PiML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from piml import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\nrequire([\"codemirror/lib/codemirror\"]);\nfunction set(str) {\n    var obj = {}, words = str.split(\" \");\n    for (var i = 0; i < words.length; ++i) obj[words[i]] = true;\n    return obj;\n  }\nvar fugue_keywords = \"fill hash rand even presort persist broadcast params process output outtransform rowcount concurrency prepartition zip print title save append parquet csv json single checkpoint weak strong deterministic yield connect sample seed take sub callback dataframe file\";\nCodeMirror.defineMIME(\"text/x-fsql\", {\n    name: \"sql\",\n    keywords: set(fugue_keywords + \" add after all alter analyze and anti archive array as asc at between bucket buckets by cache cascade case cast change clear cluster clustered codegen collection column columns comment commit compact compactions compute concatenate cost create cross cube current current_date current_timestamp database databases data dbproperties defined delete delimited deny desc describe dfs directories distinct distribute drop else end escaped except exchange exists explain export extended external false fields fileformat first following for format formatted from full function functions global grant group grouping having if ignore import in index indexes inner inpath inputformat insert intersect interval into is items join keys last lateral lazy left like limit lines list load local location lock locks logical macro map minus msck natural no not null nulls of on optimize option options or order out outer outputformat over overwrite partition partitioned partitions percent preceding principals purge range recordreader recordwriter recover reduce refresh regexp rename repair replace reset restrict revoke right rlike role roles rollback rollup row rows schema schemas select semi separated serde serdeproperties set sets show skewed sort sorted start statistics stored stratify struct table tables tablesample tblproperties temp temporary terminated then to touch transaction transactions transform true truncate unarchive unbounded uncache union unlock unset use using values view when where window with\"),\n    builtin: set(\"date datetime tinyint smallint int bigint boolean float double string binary timestamp decimal array map struct uniontype delimited serde sequencefile textfile rcfile inputformat outputformat\"),\n    atoms: set(\"false true null\"),\n    operatorChars: /^[*\\/+\\-%<>!=~&|^]/,\n    dateSQL: set(\"time\"),\n    support: set(\"ODBCdotTable doubleQuote zerolessFloat\")\n  });\n\nCodeMirror.modeInfo.push( {\n            name: \"Fugue SQL\",\n            mime: \"text/x-fsql\",\n            mode: \"sql\"\n          } );\n\nrequire(['notebook/js/codecell'], function(codecell) {\n    codecell.CodeCell.options_default.highlight_modes['magic_text/x-fsql'] = {'reg':[/%%fsql/]} ;\n    Jupyter.notebook.events.on('kernel_ready.Kernel', function(){\n    Jupyter.notebook.get_cells().map(function(cell){\n        if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n    });\n  });\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fugue_notebook import setup\n",
    "import fugue_duckdb\n",
    "import os\n",
    "import duckdb\n",
    "setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(database =':memory:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using my github repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1648913508078,
     "user": {
      "displayName": "Cole Hamilton",
      "userId": "14058711568879253144"
     },
     "user_tz": 300
    },
    "id": "trpigggiAq9l",
    "outputId": "3b18f32d-0ef6-4edc-cb8e-a7f1e56eaabd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 690 entries, 0 to 689\n",
      "Data columns (total 15 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Date Submitted          690 non-null    object \n",
      " 1   Workout Date            690 non-null    object \n",
      " 2   Activity Type           690 non-null    object \n",
      " 3   Calories Burned (kCal)  690 non-null    int64  \n",
      " 4   Distance (mi)           690 non-null    float64\n",
      " 5   Workout Time (seconds)  690 non-null    int64  \n",
      " 6   Avg Pace (min/mi)       690 non-null    float64\n",
      " 7   Max Pace                690 non-null    float64\n",
      " 8   Avg Speed (mi/h)        690 non-null    float64\n",
      " 9   Max Speed               690 non-null    float64\n",
      " 10  Avg Heart Rate          455 non-null    float64\n",
      " 11  Steps                   666 non-null    float64\n",
      " 12  Notes                   690 non-null    object \n",
      " 13  Source                  690 non-null    object \n",
      " 14  Link                    690 non-null    object \n",
      "dtypes: float64(7), int64(2), object(6)\n",
      "memory usage: 81.0+ KB\n"
     ]
    }
   ],
   "source": [
    "temp = pd.read_csv(r'https://raw.githubusercontent.com/Cbhami/Coraline/master/Resources/csv%20data/user94403143_workout_history.csv')\n",
    "temp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.to_csv('df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%fsql` not found.\n"
     ]
    }
   ],
   "source": [
    "#write my cell magic line to the notebook\n",
    "%%fsql duck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test out my SQL query\n",
    "table = con.execute('SELECT * FROM df.csv').fetchdf();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date Submitted</th>\n",
       "      <th>Workout Date</th>\n",
       "      <th>Activity Type</th>\n",
       "      <th>Calories Burned (kCal)</th>\n",
       "      <th>Distance (mi)</th>\n",
       "      <th>Workout Time (seconds)</th>\n",
       "      <th>Avg Pace (min/mi)</th>\n",
       "      <th>Max Pace</th>\n",
       "      <th>Avg Speed (mi/h)</th>\n",
       "      <th>Max Speed</th>\n",
       "      <th>Avg Heart Rate</th>\n",
       "      <th>Steps</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Source</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>June 19, 2022</td>\n",
       "      <td>June 19, 2022</td>\n",
       "      <td>Run</td>\n",
       "      <td>378</td>\n",
       "      <td>2.66</td>\n",
       "      <td>1800</td>\n",
       "      <td>11.2782</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4713.0</td>\n",
       "      <td>b''</td>\n",
       "      <td>Map My Fitness MapMyRun iPhone</td>\n",
       "      <td>http://www.mapmyfitness.com/workout/6658125994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Date Submitted   Workout Date Activity Type  Calories Burned (kCal)  \\\n",
       "0  June 19, 2022  June 19, 2022           Run                     378   \n",
       "\n",
       "   Distance (mi)  Workout Time (seconds)  Avg Pace (min/mi)  Max Pace  \\\n",
       "0           2.66                    1800            11.2782       0.0   \n",
       "\n",
       "   Avg Speed (mi/h)  Max Speed  Avg Heart Rate   Steps Notes  \\\n",
       "0              5.32        0.0             NaN  4713.0   b''   \n",
       "\n",
       "                           Source  \\\n",
       "0  Map My Fitness MapMyRun iPhone   \n",
       "\n",
       "                                             Link  \n",
       "0  http://www.mapmyfitness.com/workout/6658125994  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date Submitted', 'Workout Date', 'Activity Type',\n",
       "       'Calories Burned (kCal)', 'Distance (mi)', 'Workout Time (seconds)',\n",
       "       'Avg Pace (min/mi)', 'Max Pace', 'Avg Speed (mi/h)', 'Max Speed',\n",
       "       'Avg Heart Rate', 'Steps', 'Notes', 'Source', 'Link'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_Trim</th>\n",
       "      <th>Workout_Date</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Total_Calories</th>\n",
       "      <th>Distance</th>\n",
       "      <th>seconds</th>\n",
       "      <th>Avg_Pace</th>\n",
       "      <th>Max_Pace</th>\n",
       "      <th>Avg_Speed</th>\n",
       "      <th>Max_Speed</th>\n",
       "      <th>Average_Heart_Rate</th>\n",
       "      <th>Steps</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Source</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>June 19, 2022</td>\n",
       "      <td>June 19, 2022</td>\n",
       "      <td>Run</td>\n",
       "      <td>378</td>\n",
       "      <td>2.66000</td>\n",
       "      <td>1800</td>\n",
       "      <td>11.27820</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.32000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4713.0</td>\n",
       "      <td>b''</td>\n",
       "      <td>Map My Fitness MapMyRun iPhone</td>\n",
       "      <td>http://www.mapmyfitness.com/workout/6658125994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>June 18, 2022</td>\n",
       "      <td>June 18, 2022</td>\n",
       "      <td>Run</td>\n",
       "      <td>370</td>\n",
       "      <td>2.57857</td>\n",
       "      <td>1698</td>\n",
       "      <td>10.96610</td>\n",
       "      <td>1.11240</td>\n",
       "      <td>5.47140</td>\n",
       "      <td>53.9375</td>\n",
       "      <td>159.0</td>\n",
       "      <td>4431.0</td>\n",
       "      <td>b''</td>\n",
       "      <td>Map My Fitness MapMyRun iPhone</td>\n",
       "      <td>http://www.mapmyfitness.com/workout/6655598125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>June 16, 2022</td>\n",
       "      <td>June 17, 2022</td>\n",
       "      <td>Run</td>\n",
       "      <td>315</td>\n",
       "      <td>2.33000</td>\n",
       "      <td>1391</td>\n",
       "      <td>9.94993</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6.03019</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3684.0</td>\n",
       "      <td>b''</td>\n",
       "      <td>Map My Fitness MapMyRun iPhone</td>\n",
       "      <td>http://www.mapmyfitness.com/workout/6652076194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>June 15, 2022</td>\n",
       "      <td>June 15, 2022</td>\n",
       "      <td>Run</td>\n",
       "      <td>405</td>\n",
       "      <td>3.10000</td>\n",
       "      <td>1691</td>\n",
       "      <td>9.09140</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6.59965</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4520.0</td>\n",
       "      <td>b''</td>\n",
       "      <td>Map My Fitness MapMyRun iPhone</td>\n",
       "      <td>http://www.mapmyfitness.com/workout/6649778854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>June 12, 2022</td>\n",
       "      <td>June 12, 2022</td>\n",
       "      <td>Run</td>\n",
       "      <td>428</td>\n",
       "      <td>3.09762</td>\n",
       "      <td>1955</td>\n",
       "      <td>10.51630</td>\n",
       "      <td>3.00814</td>\n",
       "      <td>5.70545</td>\n",
       "      <td>19.9459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5052.0</td>\n",
       "      <td>b''</td>\n",
       "      <td>Map My Fitness MapMyRun iPhone</td>\n",
       "      <td>http://www.mapmyfitness.com/workout/6641879077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date_Trim   Workout_Date Activity  Total_Calories  Distance  seconds  \\\n",
       "0  June 19, 2022  June 19, 2022      Run             378   2.66000     1800   \n",
       "1  June 18, 2022  June 18, 2022      Run             370   2.57857     1698   \n",
       "2  June 16, 2022  June 17, 2022      Run             315   2.33000     1391   \n",
       "3  June 15, 2022  June 15, 2022      Run             405   3.10000     1691   \n",
       "4  June 12, 2022  June 12, 2022      Run             428   3.09762     1955   \n",
       "\n",
       "   Avg_Pace  Max_Pace  Avg_Speed  Max_Speed  Average_Heart_Rate   Steps Notes  \\\n",
       "0  11.27820   0.00000    5.32000     0.0000                 NaN  4713.0   b''   \n",
       "1  10.96610   1.11240    5.47140    53.9375               159.0  4431.0   b''   \n",
       "2   9.94993   0.00000    6.03019     0.0000                 NaN  3684.0   b''   \n",
       "3   9.09140   0.00000    6.59965     0.0000                 NaN  4520.0   b''   \n",
       "4  10.51630   3.00814    5.70545    19.9459                 0.0  5052.0   b''   \n",
       "\n",
       "                           Source  \\\n",
       "0  Map My Fitness MapMyRun iPhone   \n",
       "1  Map My Fitness MapMyRun iPhone   \n",
       "2  Map My Fitness MapMyRun iPhone   \n",
       "3  Map My Fitness MapMyRun iPhone   \n",
       "4  Map My Fitness MapMyRun iPhone   \n",
       "\n",
       "                                             Link  \n",
       "0  http://www.mapmyfitness.com/workout/6658125994  \n",
       "1  http://www.mapmyfitness.com/workout/6655598125  \n",
       "2  http://www.mapmyfitness.com/workout/6652076194  \n",
       "3  http://www.mapmyfitness.com/workout/6649778854  \n",
       "4  http://www.mapmyfitness.com/workout/6641879077  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply the TRIM function to column headers\n",
    "df = con.execute('SELECT TRIM(BOTH ' ' FROM \"Date Submitted\")  AS Date_Trim, \"Workout Date\" AS Workout_Date, TRIM(BOTH ' ' FROM \"Activity Type\")  AS Activity, \"Calories Burned (kCal)\" AS Total_Calories, \"Distance (mi)\" AS Distance, \"Workout Time (Seconds)\" AS Seconds, \"Avg Pace (min/mi)\" AS Avg_Pace, \"Max Pace\" AS Max_Pace, \"Avg Speed (mi/h)\" AS Avg_Speed, \"Max Speed\" AS Max_Speed, \"Avg Heart Rate\" AS Average_Heart_Rate, Steps, Notes, Source, Link FROM df.csv WHERE Distance > 0').fetchdf()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date_Trim', 'Workout_Date', 'Activity', 'Total_Calories', 'Distance',\n",
       "       'seconds', 'Avg_Pace', 'Max_Pace', 'Avg_Speed', 'Max_Speed',\n",
       "       'Average_Heart_Rate', 'Steps', 'Notes', 'Source', 'Link'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.value_counts().sum()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = con.execute('ALTER TABLE CHANGE COLUMN \"Workout Date\" TO \"Date\" FROM df.csv').fetchdf()\n",
    "# df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_duckdb_extension.DuckDBPyConnection at 0x2f3324656b0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get my table name\n",
    "con.execute('SHOW TABLES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total_Calories</th>\n",
       "      <th>Distance</th>\n",
       "      <th>seconds</th>\n",
       "      <th>Avg_Pace</th>\n",
       "      <th>Max_Pace</th>\n",
       "      <th>Avg_Speed</th>\n",
       "      <th>Max_Speed</th>\n",
       "      <th>Average_Heart_Rate</th>\n",
       "      <th>Steps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>667.000000</td>\n",
       "      <td>667.000000</td>\n",
       "      <td>667.000000</td>\n",
       "      <td>667.000000</td>\n",
       "      <td>667.000000</td>\n",
       "      <td>667.000000</td>\n",
       "      <td>667.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>666.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>408.187406</td>\n",
       "      <td>3.187186</td>\n",
       "      <td>2450.517241</td>\n",
       "      <td>13.508088</td>\n",
       "      <td>2.081515</td>\n",
       "      <td>5.645313</td>\n",
       "      <td>47.891159</td>\n",
       "      <td>109.918552</td>\n",
       "      <td>5517.382883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>282.046856</td>\n",
       "      <td>1.926695</td>\n",
       "      <td>4796.220920</td>\n",
       "      <td>19.100333</td>\n",
       "      <td>7.659103</td>\n",
       "      <td>1.536980</td>\n",
       "      <td>135.104480</td>\n",
       "      <td>61.336976</td>\n",
       "      <td>9935.677622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.015731</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>252.500000</td>\n",
       "      <td>2.034545</td>\n",
       "      <td>1480.000000</td>\n",
       "      <td>9.130750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.308200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>95.500000</td>\n",
       "      <td>3307.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>403.000000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>9.763800</td>\n",
       "      <td>0.872705</td>\n",
       "      <td>6.142910</td>\n",
       "      <td>19.237100</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>4649.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>457.000000</td>\n",
       "      <td>3.557410</td>\n",
       "      <td>2341.000000</td>\n",
       "      <td>11.300550</td>\n",
       "      <td>1.690110</td>\n",
       "      <td>6.569745</td>\n",
       "      <td>60.548250</td>\n",
       "      <td>153.750000</td>\n",
       "      <td>5742.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2473.000000</td>\n",
       "      <td>13.481600</td>\n",
       "      <td>80568.000000</td>\n",
       "      <td>295.261503</td>\n",
       "      <td>182.396000</td>\n",
       "      <td>14.863700</td>\n",
       "      <td>1853.550000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>193886.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Total_Calories    Distance       seconds    Avg_Pace    Max_Pace  \\\n",
       "count      667.000000  667.000000    667.000000  667.000000  667.000000   \n",
       "mean       408.187406    3.187186   2450.517241   13.508088    2.081515   \n",
       "std        282.046856    1.926695   4796.220920   19.100333    7.659103   \n",
       "min          2.000000    0.015731      0.000000    0.000000    0.000000   \n",
       "25%        252.500000    2.034545   1480.000000    9.130750    0.000000   \n",
       "50%        403.000000    3.100000   1800.000000    9.763800    0.872705   \n",
       "75%        457.000000    3.557410   2341.000000   11.300550    1.690110   \n",
       "max       2473.000000   13.481600  80568.000000  295.261503  182.396000   \n",
       "\n",
       "        Avg_Speed    Max_Speed  Average_Heart_Rate          Steps  \n",
       "count  667.000000   667.000000          442.000000     666.000000  \n",
       "mean     5.645313    47.891159          109.918552    5517.382883  \n",
       "std      1.536980   135.104480           61.336976    9935.677622  \n",
       "min      0.000000     0.000000            0.000000       6.000000  \n",
       "25%      5.308200     0.000000           95.500000    3307.000000  \n",
       "50%      6.142910    19.237100          143.500000    4649.500000  \n",
       "75%      6.569745    60.548250          153.750000    5742.000000  \n",
       "max     14.863700  1853.550000          169.000000  193886.000000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 119,
     "status": "ok",
     "timestamp": 1648913514136,
     "user": {
      "displayName": "Cole Hamilton",
      "userId": "14058711568879253144"
     },
     "user_tz": 300
    },
    "id": "2uXi8KYxAq9o"
   },
   "outputs": [],
   "source": [
    "# headers_dict = {'Workout Date': 'Workout_Date', 'Avg Pace (min/mi)': 'Avg_Pace', 'Distance (mi)':'Distance', 'Avg Speed (mi/h)':'Avg_Speed', 'Avg Heart Rate':'Avg_Heart_Rate', 'Calories Burned (kCal)':'Calories_Burned', 'Workout Time (seconds)': 'Workout_Time', 'Max Pace (min/mi)':'Max_Pace', 'Max Pace':'Max_Pace', 'Date Submitted':'Date_Submitted', 'Activity Type':'Activity_Type', 'Max Speed (mi/h)':'Max_Speed'}\n",
    "# temp.rename(columns = headers_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date_Trim', 'Workout_Date', 'Activity', 'Total_Calories', 'Distance',\n",
       "       'seconds', 'Avg_Pace', 'Max_Pace', 'Avg_Speed', 'Max_Speed',\n",
       "       'Average_Heart_Rate', 'Steps', 'Notes', 'Source', 'Link'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 667 entries, 0 to 666\n",
      "Data columns (total 16 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   Date_Trim           667 non-null    datetime64[ns]\n",
      " 1   Workout_Date        667 non-null    datetime64[ns]\n",
      " 2   Activity            667 non-null    object        \n",
      " 3   Total_Calories      667 non-null    int32         \n",
      " 4   Distance            667 non-null    float64       \n",
      " 5   seconds             667 non-null    int32         \n",
      " 6   Avg_Pace            667 non-null    float64       \n",
      " 7   Max_Pace            667 non-null    float64       \n",
      " 8   Avg_Speed           667 non-null    float64       \n",
      " 9   Max_Speed           667 non-null    float64       \n",
      " 10  Average_Heart_Rate  442 non-null    float64       \n",
      " 11  Steps               666 non-null    float64       \n",
      " 12  Notes               667 non-null    object        \n",
      " 13  Source              667 non-null    object        \n",
      " 14  Link                667 non-null    object        \n",
      " 15  day_of_week         667 non-null    object        \n",
      "dtypes: datetime64[ns](2), float64(7), int32(2), object(5)\n",
      "memory usage: 78.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df['Workout_Date'] = pd.to_datetime(df['Workout_Date'])\n",
    "df['Date_Trim'] = pd.to_datetime(df['Date_Trim'])\n",
    "mean_HR = df['Average_Heart_Rate'].mean()\n",
    "#temp['Avg_Heart_Rate'].fillna(value = mean_value, inplace = True)\n",
    "\n",
    "#step_value = temp['Steps'].mean()\n",
    "#temp['Steps'].fillna(value = step_value, inplace = True)\n",
    "df['day_of_week']=df['Workout_Date'].dt.day_name()\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Average_Heart_Rate == 0] = np.nan\n",
    "#temp[temp.Max_Pace == 0] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_Trim</th>\n",
       "      <th>Workout_Date</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Total_Calories</th>\n",
       "      <th>Distance</th>\n",
       "      <th>seconds</th>\n",
       "      <th>Avg_Pace</th>\n",
       "      <th>Max_Pace</th>\n",
       "      <th>Avg_Speed</th>\n",
       "      <th>Max_Speed</th>\n",
       "      <th>Average_Heart_Rate</th>\n",
       "      <th>Steps</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Source</th>\n",
       "      <th>Link</th>\n",
       "      <th>day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-06-19</td>\n",
       "      <td>2022-06-19</td>\n",
       "      <td>Run</td>\n",
       "      <td>378.0</td>\n",
       "      <td>2.66</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>11.2782</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>109.918552</td>\n",
       "      <td>4713.0</td>\n",
       "      <td>b''</td>\n",
       "      <td>Map My Fitness MapMyRun iPhone</td>\n",
       "      <td>http://www.mapmyfitness.com/workout/6658125994</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Date_Trim Workout_Date Activity  Total_Calories  Distance  seconds  \\\n",
       "0 2022-06-19   2022-06-19      Run           378.0      2.66   1800.0   \n",
       "\n",
       "   Avg_Pace  Max_Pace  Avg_Speed  Max_Speed  Average_Heart_Rate   Steps Notes  \\\n",
       "0   11.2782       0.0       5.32        0.0          109.918552  4713.0   b''   \n",
       "\n",
       "                           Source  \\\n",
       "0  Map My Fitness MapMyRun iPhone   \n",
       "\n",
       "                                             Link day_of_week  \n",
       "0  http://www.mapmyfitness.com/workout/6658125994      Sunday  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Average_Heart_Rate'].fillna(mean_HR, inplace = True)\n",
    "#temp.fillna(temp.Max_Pace.median(), inplace = True)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1648913601365,
     "user": {
      "displayName": "Cole Hamilton",
      "userId": "14058711568879253144"
     },
     "user_tz": 300
    },
    "id": "elgB3t5aAq9p",
    "outputId": "45146285-6a9d-4412-c661-38353e247313"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date_Trim             457\n",
       "Workout_Date          457\n",
       "Activity              457\n",
       "Total_Calories        457\n",
       "Distance              457\n",
       "seconds               457\n",
       "Avg_Pace              457\n",
       "Avg_Speed             457\n",
       "Average_Heart_Rate    457\n",
       "Steps                 457\n",
       "day_of_week           457\n",
       "Calculated            457\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = pd.DataFrame(df)\n",
    "temp_df.drop(['Notes', 'Source', 'Link', 'Max_Speed', 'Max_Pace'], axis = 1, inplace = True)\n",
    "#temp_df.rename(columns = headers_dict, inplace=True)\n",
    "upd = temp_df.round(2)\n",
    "upd2 =upd\n",
    "upd3 = upd2[upd2['Avg_Pace'].between(6, 13)]\n",
    "upd4 = upd3[upd3['Avg_Speed'] > 3]\n",
    "upd4.sort_values(by=['Workout_Date'], ascending=True)\n",
    "#upd5 = upd4[upd4['Avg_Pace'] < 13]\n",
    "df = upd4[upd4['Activity'] == 'Run']\n",
    "df['Calculated'] = df['seconds'] / 60\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_Trim</th>\n",
       "      <th>Workout_Date</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Total_Calories</th>\n",
       "      <th>Distance</th>\n",
       "      <th>seconds</th>\n",
       "      <th>Avg_Pace</th>\n",
       "      <th>Avg_Speed</th>\n",
       "      <th>Average_Heart_Rate</th>\n",
       "      <th>Steps</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>Calculated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-06-19</td>\n",
       "      <td>2022-06-19</td>\n",
       "      <td>Run</td>\n",
       "      <td>378.0</td>\n",
       "      <td>2.66</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>11.28</td>\n",
       "      <td>5.32</td>\n",
       "      <td>109.92</td>\n",
       "      <td>4713.0</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-06-18</td>\n",
       "      <td>2022-06-18</td>\n",
       "      <td>Run</td>\n",
       "      <td>370.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>1698.0</td>\n",
       "      <td>10.97</td>\n",
       "      <td>5.47</td>\n",
       "      <td>159.00</td>\n",
       "      <td>4431.0</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>28.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-06-16</td>\n",
       "      <td>2022-06-17</td>\n",
       "      <td>Run</td>\n",
       "      <td>315.0</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1391.0</td>\n",
       "      <td>9.95</td>\n",
       "      <td>6.03</td>\n",
       "      <td>109.92</td>\n",
       "      <td>3684.0</td>\n",
       "      <td>Friday</td>\n",
       "      <td>23.183333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-06-15</td>\n",
       "      <td>2022-06-15</td>\n",
       "      <td>Run</td>\n",
       "      <td>405.0</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1691.0</td>\n",
       "      <td>9.09</td>\n",
       "      <td>6.60</td>\n",
       "      <td>109.92</td>\n",
       "      <td>4520.0</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>28.183333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-06-10</td>\n",
       "      <td>2022-06-10</td>\n",
       "      <td>Run</td>\n",
       "      <td>881.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>4182.0</td>\n",
       "      <td>11.23</td>\n",
       "      <td>5.34</td>\n",
       "      <td>153.00</td>\n",
       "      <td>10479.0</td>\n",
       "      <td>Friday</td>\n",
       "      <td>69.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>2019-07-05</td>\n",
       "      <td>2019-07-05</td>\n",
       "      <td>Run</td>\n",
       "      <td>156.0</td>\n",
       "      <td>1.52</td>\n",
       "      <td>843.0</td>\n",
       "      <td>9.24</td>\n",
       "      <td>6.49</td>\n",
       "      <td>160.00</td>\n",
       "      <td>2313.0</td>\n",
       "      <td>Friday</td>\n",
       "      <td>14.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>2019-07-04</td>\n",
       "      <td>2019-07-04</td>\n",
       "      <td>Run</td>\n",
       "      <td>156.0</td>\n",
       "      <td>1.53</td>\n",
       "      <td>816.0</td>\n",
       "      <td>8.88</td>\n",
       "      <td>6.76</td>\n",
       "      <td>159.00</td>\n",
       "      <td>2215.0</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>13.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>2019-07-02</td>\n",
       "      <td>2019-07-02</td>\n",
       "      <td>Run</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.02</td>\n",
       "      <td>552.0</td>\n",
       "      <td>9.02</td>\n",
       "      <td>6.65</td>\n",
       "      <td>148.00</td>\n",
       "      <td>1439.0</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>9.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>2019-06-29</td>\n",
       "      <td>2019-06-29</td>\n",
       "      <td>Run</td>\n",
       "      <td>118.0</td>\n",
       "      <td>1.27</td>\n",
       "      <td>693.0</td>\n",
       "      <td>9.13</td>\n",
       "      <td>6.57</td>\n",
       "      <td>150.00</td>\n",
       "      <td>1824.0</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>11.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>2019-05-19</td>\n",
       "      <td>2019-05-19</td>\n",
       "      <td>Run</td>\n",
       "      <td>419.0</td>\n",
       "      <td>3.11</td>\n",
       "      <td>2197.0</td>\n",
       "      <td>11.69</td>\n",
       "      <td>5.13</td>\n",
       "      <td>139.00</td>\n",
       "      <td>5295.0</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>36.616667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>457 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Date_Trim Workout_Date Activity  Total_Calories  Distance  seconds  \\\n",
       "0   2022-06-19   2022-06-19      Run           378.0      2.66   1800.0   \n",
       "1   2022-06-18   2022-06-18      Run           370.0      2.58   1698.0   \n",
       "2   2022-06-16   2022-06-17      Run           315.0      2.33   1391.0   \n",
       "3   2022-06-15   2022-06-15      Run           405.0      3.10   1691.0   \n",
       "5   2022-06-10   2022-06-10      Run           881.0      6.20   4182.0   \n",
       "..         ...          ...      ...             ...       ...      ...   \n",
       "635 2019-07-05   2019-07-05      Run           156.0      1.52    843.0   \n",
       "637 2019-07-04   2019-07-04      Run           156.0      1.53    816.0   \n",
       "639 2019-07-02   2019-07-02      Run            95.0      1.02    552.0   \n",
       "642 2019-06-29   2019-06-29      Run           118.0      1.27    693.0   \n",
       "663 2019-05-19   2019-05-19      Run           419.0      3.11   2197.0   \n",
       "\n",
       "     Avg_Pace  Avg_Speed  Average_Heart_Rate    Steps day_of_week  Calculated  \n",
       "0       11.28       5.32              109.92   4713.0      Sunday   30.000000  \n",
       "1       10.97       5.47              159.00   4431.0    Saturday   28.300000  \n",
       "2        9.95       6.03              109.92   3684.0      Friday   23.183333  \n",
       "3        9.09       6.60              109.92   4520.0   Wednesday   28.183333  \n",
       "5       11.23       5.34              153.00  10479.0      Friday   69.700000  \n",
       "..        ...        ...                 ...      ...         ...         ...  \n",
       "635      9.24       6.49              160.00   2313.0      Friday   14.050000  \n",
       "637      8.88       6.76              159.00   2215.0    Thursday   13.600000  \n",
       "639      9.02       6.65              148.00   1439.0     Tuesday    9.200000  \n",
       "642      9.13       6.57              150.00   1824.0    Saturday   11.550000  \n",
       "663     11.69       5.13              139.00   5295.0      Sunday   36.616667  \n",
       "\n",
       "[457 rows x 12 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Workout_Date'] = pd.to_datetime(df['Workout_Date'])\n",
    "#df.drop(['Max_Pace', 'Max Speed'])\n",
    "df.sort_values(by=['Workout_Date'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "executionInfo": {
     "elapsed": 1293,
     "status": "ok",
     "timestamp": 1648913538796,
     "user": {
      "displayName": "Cole Hamilton",
      "userId": "14058711568879253144"
     },
     "user_tz": 300
    },
    "id": "iA14D4LUAq9q",
    "outputId": "25f23056-148b-44e1-e057-f2f375f15f90"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "axes = df.plot(x='Distance', y='Calculated', style='.',figsize=(15,9))\n",
    "y_label = axes.set_ylabel('Calculated_Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxSFTuz3Aq96"
   },
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5G2aByctAq96"
   },
   "outputs": [],
   "source": [
    "linear_regression = stats.linregress(x=df.Calculated,y=df.Distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 764,
     "status": "ok",
     "timestamp": 1648751531292,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "WKO1vzEdAq97",
    "outputId": "3eb55d71-1b14-4c92-973e-1317229a87f0"
   },
   "outputs": [],
   "source": [
    "linear_regression.slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1648764241411,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "NcM7c87nAq98",
    "outputId": "0bd210d5-0cb7-4114-8351-94f9a9dd24b2"
   },
   "outputs": [],
   "source": [
    "linear_regression.intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C38ZNcIhAq99"
   },
   "source": [
    "* We can use these values with the simple linear regression equation for a straight line to predict the average January temperature in New York City for any given year.\n",
    "* In the following calculation, `linear_regression.slope` is **_m_**, our input year is **_x_** (the date value for which you’d like to predict the temperature), and `linear_regression.intercept` is **_b_**.\n",
    "* We can also predict the approximate difference between one year and another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1648764249382,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "Nqh3UA0MAq9-",
    "outputId": "96fac6bd-58a7-4ed0-9359-6b2d7f9bfe55"
   },
   "outputs": [],
   "source": [
    "Half = linear_regression.slope * 13.1 + linear_regression.intercept \n",
    "print(Half) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1648764253577,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "HosgygIPAq9-",
    "outputId": "59ed71b3-9fa0-4f00-84a7-e04ef5e96db7"
   },
   "outputs": [],
   "source": [
    "Full = linear_regression.slope * 26.2 + linear_regression.intercept\n",
    "print(Full-Half)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3o3ZMGEAq9-",
    "tags": []
   },
   "source": [
    "### Plotting the Regression Line\n",
    "* Seaborn’s **`regplot` function** plots each data point with one attribute on the **_x_****-axis and the other on the **_y_**-axis\n",
    "* Creates a **scatter plot** or **scattergram** representing the `Temperature`s for the given `Date`s and adds the regression line\n",
    "* Function `regplot`’s `x` and `y` keyword arguments are one-dimensional arrays of the same length representing the **_x-y_** coordinate pairs to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMTK6JckAq9_"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "executionInfo": {
     "elapsed": 437,
     "status": "ok",
     "timestamp": 1648764832305,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "4CW71baTAq9_",
    "outputId": "746ed503-f8d6-40b2-c807-c2e609bb914d"
   },
   "outputs": [],
   "source": [
    "axes = sns.regplot(x=df.Distance, y=df.Calculated)\n",
    "axes.set_ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## EDA FOR PLOTTING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show data of one subject\n",
    "mask1 = df['Activity_Type'] == 'Run'\n",
    "(\n",
    "    df[mask1]\n",
    "    .set_index('Workout_Date')\n",
    "    ['Calculated']\n",
    "    .plot()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot the average of `Calculated` data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic approach to plot using matplotlib w/ chaining approach\n",
    "(\n",
    "    df\n",
    "    .set_index('day_of_week')\n",
    "    ['Calculated']\n",
    "    .plot()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check runs for 2022\n",
    "(\n",
    "    df\n",
    "    .set_index('Workout_Date')\n",
    "    .query('Workout_Date > 20220101')\n",
    "    ['Distance']\n",
    "    .plot()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the agg of a col\n",
    "group = df.groupby(\n",
    "    'Date_Submitted').count(\n",
    "\n",
    "    ).sort_values(\n",
    "        by=['Workout_Date'],\n",
    "         ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correlation matrix / Pairplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap - correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the correlation matrix\n",
    "df.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default heatmap\n",
    "p1 = sns.heatmap(\n",
    "    df\n",
    "    .corr(),\n",
    "    annot=True,\n",
    "    annot_kws={\"size\": 7}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show columns\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression b/w steps count and movement intensity\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()\n",
    "\n",
    "# plot\n",
    "sns.regplot(x=df[\"Calculated\"], y=df[\"Calories_Burned\"],\n",
    "            line_kws={\"color\": \"green\", \"alpha\": 0.7, \"lw\": 5})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression b/w steps count and movement intensity\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot\n",
    "sns.regplot(x=df[\"Avg_Speed\"], y=df[\"Calories_Burned\"],\n",
    "            line_kws={\"color\": \"black\", \"alpha\": 0.7, \"lw\": 5})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = df['Steps'] == 0  # get the rows where steps count is 0\n",
    "df[~mask1]['Steps'].hist(bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[['Workout_Date', 'Calories_Burned',\n",
    "       'Distance', 'Workout_Time', 'Avg_Pace', 'Avg_Speed',\n",
    "       'Avg_Heart_Rate', 'Steps',\n",
    "       'Calculated', 'day_of_week']]\n",
    "df2.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = df2.select_dtypes(include = ['float64', 'int64'])\n",
    "df_num_corr = df_num.corr()['Distance'][:-1] # -1 means that the latest row is SalePrice\n",
    "top_features = df_num_corr[abs(df_num_corr) > 0.5].sort_values(ascending=False) #displays pearsons correlation coefficient greater than 0.5\n",
    "print(\"There are {} strongly correlated values with Distance:\\n{}\".format(len(top_features), top_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Looking for Correlations**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding with the data cleaning, it is useful to establish a correlation between the response variable (in our case the sale price) and other predictor variables, as some of them might not have any major impact in determining the price of the house and will not be used in the analysis.  There are many ways to discover correlation between the target variable and the rest of the features. Building pair plots, scatter plots, heat maps, and a correlation matrixes are the most common ones. Below, we will use the `corr()` function to list the top features based on the [pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01) (measures how closely two sequences of numbers are correlated). Correlation coefficient can only be calculated on the numerical attributes (floats and integers), therefore, only the numeric attributes will be selected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's generate some par plots to visually inspect the correlation between some of these features and the target variable. We will use seaborns `sns.pairplot()` function for this analysis. Also, building pair plots is one of the possible ways to spot the outliers that might be present in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(df_num.columns), 5):\n",
    "    sns.pairplot(data=df_num,\n",
    "                x_vars=df_num.columns[i:i+5],\n",
    "                y_vars=['Distance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Pearsons Correlation Coefficients and pair plots, we can draw some conclusions about the features that are most strongly correlated to the 'SalePrice'. They are: 'Overall Qual', 'Gr Liv Area', 'Garage Cars', 'Garage Area', and others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Log Transformation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to inspect whether our 'SalePrice' data are normally distributed. The assumption of the normal distribution must be met in order to perform any type of regression analysis. There are several ways to check for this assumption, however here, we will use the visual method, by plotting the 'Distance' distribution using the `distplot()` function from the `seaborn` library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_untransformed = sns.distplot(df2['Distance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the plot shows, 'Distance' deviates from the normal distribution. It has a longer tail to the right, so we call it a positive skew. In statistics *skewness* is a measure of asymmetry of the distribution. In addition to skewness, there is also a kurtosis, parameter which refers to the pointedness of a peak in the distribution curve. Both skewness and kurtosis are frequently used together to characterize the distribution of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skewness: %f\" % df2['Distance'].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of skewness for a fairly symmetrical bell curve distribution is between -0.5 and 0.5; moderate skewness is -0.5 to -1.0 and 0.5 to 1.0; and highly skewed distribution is < -1.0 and > 1.0. In our case, we have \\~1.7, so it is considered  highly skewed data.\n",
    "\n",
    "Now, we can try to transform our data, so it looks more normally distributed. We can use the `np.log()` function from the `numpy` library to perform log transform. This [documentation](https://numpy.org/doc/stable/reference/generated/numpy.log.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01) contains more information about the numpy log transform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_transformed = np.log(df2['Distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = sns.distplot(log_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the log method transformed the 'Distance' distribution into a more symmetrical bell curve and the skewness level now is -0.01, well within the range.\n",
    "\n",
    "There are other ways to correct for skewness of the data. For example, Square Root Transform (`np.sqrt`) and the Box-Cox Transform (`stats.boxcox` from the `scipy stats` library). To learn more about these two methods, please check out this [article](https://towardsdatascience.com/top-3-methods-for-handling-skewed-data-1334e0debf45?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skewness: %f\" % (log_transformed).skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.index.is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Scaling**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important transformations we need to apply to our data is feature scaling.  There are two common ways to get all attributes to have the same scale: min-max scaling and standardization.\n",
    "\n",
    "Min-max scaling (or normalization) is the simplest: values are shifted and rescaled so they end up ranging from 0 to 1. This is done by subtracting the min value and dividing by the max minus min.\n",
    "\n",
    "Standardization is different: first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation, so that the resulting distribution has unit variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data = MinMaxScaler().fit_transform(df_num)\n",
    "norm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the data is now a `ndarray`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also standardize our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = StandardScaler().fit_transform(df_num)\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Handling the Outliers**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, an outlier is an observation point that is distant from other observations. An outlier can be due to some mistakes in data collection or recording, or due to natural high variability of data points. How to treat an outlier highly depends on our data or the type of analysis to be performed. Outliers can markedly affect our models and can be a valuable source of information, providing us insights about specific behaviours.\n",
    "\n",
    "There are many ways to discover outliers in our data. We can do Uni-variate analysis (using one variable analysis) or Multi-variate analysis (using two or more variables). One of the simplest ways to detect an outlier is to inspect the data visually, by making box plots or scatter plots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uni-variate Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A box plot is a method for graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles. Outliers may be plotted as individual points. To learn more about box plots please click [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.boxplot.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df2['Distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df2['Avg_Speed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df2['Avg_Heart_Rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-variate Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will look at the bi-variate analysis of the two features, the sale price, 'SalePrice', and the ground living area, 'GrLivArea', and plot the scatter plot of the relationship between these two parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_area = df2.plot.scatter(x='Avg_Heart_Rate',\n",
    "                      y='Avg_Pace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting the Outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will sort all of our 'Gr Liv Area' values and select only the last two.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.sort_values(by = 'Max Speed', ascending = False)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outliers_dropped = df.drop([df.index[[529,159]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z-score Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z-score is another way to identify outliers mathematically. Z-score is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured. In another words, Z-score is the value that quantifies relationship between a data point and a standard deviation and mean values of a group of points. Data points which are too far from zero will be treated as the outliers. In most of the cases, a threshold of 3 or -3 is used. For example, if the Z-score value is greater than or less than 3 or -3 standard deviations respectively, that data point will be identified as a outlier.\n",
    "\n",
    "To learn more about Z-score, please visit this [Wikipedia](https://en.wikipedia.org/wiki/Standard_score?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01) site.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we are using Z-score function from `scipy` library to detect the outliers in our 'Low Qual Fin SF' parameter. To learn more about `scipy.stats`, please visit this [link](https://docs.scipy.org/doc/scipy/reference/tutorial/stats.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2['Distance'] = stats.zscore(df2['Distance'])\n",
    "# df2[['Distance','Distance']].describe().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaled results show a mean of 0.000 and a standard deviation of 1.000, indicating that the transformed values fit the z-scale model. The max value of 5.084 is further proof of the presence of outliers, as it falls well above the z-score limit of +3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null values before testing/training\n",
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the sample function pulls a random sample from the dataframe\n",
    "\n",
    "sample = df.sample(n=10, replace=False)\n",
    "print(sample.iloc[:,4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df2,\n",
    "hue='Distance', size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=df2['Distance'],\n",
    "y=df2['Avg_Pace'], kind='hex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot = sns.FacetGrid(df2, col='Distance',\n",
    "# margin_titles=True)\n",
    "# plot.map(plt.hist, 'Avg_Heart_Rate', color = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Part Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out how many entries there are in our dataset, using `shape` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `info` function, we will take a look at our types of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `columns` method, we will print all the column names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we will check for any missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Wrangling**\n",
    "\n",
    "### Selecting and renaming the columns of interest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we are filtering our data, by selecting only the relevant columns. Also, we are using the `rename()` method to change the name of the columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = (df2[['Workout_Date','Calories_Burned','Distance','Workout_Time', 'Avg_Pace', 'Avg_Speed', 'Avg_Heart_Rate', 'Steps', 'Calculated', 'day_of_week']]).rename(columns={\"Calculated\" : \"Minutes\"})\n",
    "df3[['Minutes']] = df3[['Minutes']].round(2)\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create four separate plots, use Pandas `.hist` method\n",
    "axList = df3.hist(bins=25)\n",
    "\n",
    "# Add some x- and y- labels to first column and last row\n",
    "for ax in axList.flatten():\n",
    "    if ax.is_last_row():\n",
    "        ax.set_xlabel('Distribution')\n",
    "        \n",
    "    if ax.is_first_col():\n",
    "        ax.set_ylabel('Count')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "# This uses the `.plot.hist` method\n",
    "ax = df3.plot.hist(bins=20, alpha=0.5)\n",
    "ax.set_xlabel('Category');\n",
    "plt.title('Category Histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas boxplot\n",
    "\n",
    "Using Pandas, make a boxplot of each measurement. Here is the documentation for [Pandas boxplot method](http://pandas.pydata.org/pandas-docs/version/0.18.1/visualization.html#visualization-box)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.boxplot(by='day_of_week')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['day_of_week']=df3['Workout_Date'].dt.day_name()\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_values(axs, orient=\"v\", space=.01):\n",
    "    def _single(ax):\n",
    "        if orient == \"v\":\n",
    "            for p in ax.patches:\n",
    "                _x = p.get_x() + p.get_width() / 2\n",
    "                _y = p.get_y() + p.get_height() + (p.get_height()*0.01)\n",
    "                value = '{:.1f}'.format(p.get_height())\n",
    "                ax.text(_x, _y, value, ha=\"center\") \n",
    "        elif orient == \"h\":\n",
    "            for p in ax.patches:\n",
    "                _x = p.get_x() + p.get_width() + float(space)\n",
    "                _y = p.get_y() + p.get_height() - (p.get_height()*0.5)\n",
    "                value = '{:.1f}'.format(p.get_width())\n",
    "                ax.text(_x, _y, value, ha=\"left\")\n",
    "\n",
    "    if isinstance(axs, np.ndarray):\n",
    "        for idx, ax in np.ndenumerate(axs):\n",
    "            _single(ax)\n",
    "    else:\n",
    "        _single(axs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = sns.barplot(x='day_of_week',y='Distance', data=df3)\n",
    "plt.ylabel('Distance')\n",
    "plt.xlabel('Day')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.title('Avg_Distance by Day')\n",
    "show_values(days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")\n",
    "\n",
    "sns.barplot(y='Distance', x=pd.cut(\n",
    "    df3['Minutes'], bins = 5),\n",
    "    data=df2, ci=None)\n",
    "\n",
    "plt.ylabel('Distance')\n",
    "plt.xlabel('Minutes')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.title('Minutes by Distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairplot = df3[['Distance', 'Steps', 'Minutes', 'Calories_Burned']]\n",
    "sns.pairplot(pairplot, hue='Steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "sns.set_context(\"poster\")\n",
    "jplot = sns.jointplot(x=df3['Distance'], y=df3['Minutes'], kind='hex')\n",
    "#jplot.fig.suptitle('Minutes by Steps', loc = 'right', fontsize = 14)\n",
    "plt.ylabel('Minutes', fontsize = 13)\n",
    "plt.xlabel('Distance', fontsize = 13)\n",
    "#plt.xticks(rotation = 45)\n",
    "jplot.fig.set_figwidth(9)\n",
    "jplot.fig.set_figheight(9)\n",
    "plt.title('Minutes by Distance', y = 1.2, loc = 'left', fontsize = 14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={\"figure.figsize\":(16, 16)}) #width=8, height=4\n",
    "jbox = sns.boxplot(x = 'Distance', y='day_of_week', data=df3, orient = 'h', hue = (pd.cut(df3['Avg_Heart_Rate'], bins = 5)))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.ylabel('Day of Week', fontsize=22)\n",
    "plt.xlabel('Distance', fontsize=22)\n",
    "plt.yticks(fontsize = 20)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.legend(fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric vs. Non-parametric\n",
    "\n",
    "Parametric (finite number of distributions)\n",
    "\n",
    "Non-parametric we make fewer assumptions\n",
    "\n",
    "Maximum Likelihood Estimation: Related to probability and is a function of the parameters model.\n",
    "\n",
    "Uniform distribution (uniform because there is an equal chance you will get any of the values as an output of the distribution)\n",
    "\n",
    "Gaussian/Normal - Most likely value is the value closest to the mean. Further out values are equally unlikely.\n",
    "\n",
    "Central limit theorum - take the average value from random samples. The distribution of those averages will be a normal curve. \n",
    "\n",
    "Log Normal - If you take the log of a variable you will have the normal distribution.\n",
    "\n",
    "Exponential Curve - Most values closer to the left side. Often used to same what will be the amount of time before the next event.\n",
    "\n",
    "Poisson - The number of events that happen during a certain amount of time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequentist - Repeated observations in the limit.\n",
    "\n",
    "Queueing theory - study of working with queues or lines. How much supply do we need to handle the demand (web servers for frequencies, waiters for customers at a restaurant)\n",
    "\n",
    "Processes may have true frequencies - we are interested in modeling as many repeats of an experiment as possible. If the sample is large enough, we will have seen enough queues or lines to infer an estimate of our probabilities.\n",
    "\n",
    "Derive the probalistic property of a procedure.\n",
    "\n",
    "The more data we have the more confident we can be.\n",
    "\n",
    "Apply the probability directly to the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian - Describes parameters by probability distributions\n",
    "\n",
    "Before seeing any data, a prior distribution is formulated. X amount of people in line at a certain time period will allow us to have an estimated guess.\n",
    "\n",
    "Prior distributed is updated after seeing the data. Our initial estimate updates once data is introduced.\n",
    "\n",
    "The updated data is referred to as posterior distribution.\n",
    "\n",
    "Same math and same data - the differences is the interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set_style('white')\n",
    "# sns.set_context('notebook')\n",
    "# sns.set_palette('dark')\n",
    "\n",
    "# f = plt.figure(figsize=(6,4))\n",
    "# sns.boxplot(x='Distance', y='Avg_Heart_Rate', \n",
    "#             hue='Avg_Pace', data=df2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.Avg_Heart_Rate.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import normaltest\n",
    "normaltest(df3.Distance.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "sns.pairplot(df3, hue='Distance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather x, y; Train the model by finding the best prediction\n",
    "\n",
    "Focus on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation of Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import log\n",
    "from scipy.stats import boxcox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from helper import (plot_exponential_data, \n",
    "                    plot_square_normal_data)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.drop(['Workout_Date', 'day_of_week'], axis = 1)\n",
    "df4.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.Distance.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does not look normal due to that right tail. Let's try to verify statistically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import normaltest # D'Agostino K^2 Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without getting into Bayesian vs. frequentist debates, for the purposes of this lesson, the following will suffice:\n",
    "\n",
    "* This is a statistical test that tests whether a distribution is normally distributed or not. It isn't perfect, but suffice it to say: \n",
    "    * This test outputs a \"p-value\". The _higher_ this p-value is the _closer_ the distribution is to normal.\n",
    "    * Frequentist statisticians would say that you accept that the distribution is normal (more specifically: fail to reject the null hypothesis that it is normal) if p > 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normaltest(df4.Distance.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p-value _extremely_ low. Our y variable we've been dealing with this whole time was not normally distributed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression assumes a normally distributed residuals which can be aided by transforming y variable. Let's try some common transformations to try and get y to be normally distributed: \n",
    "\n",
    "* Log\n",
    "* Square root\n",
    "* Box cox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log transform can transform data that is significantly skewed right to be more normally distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dist = np.log(df4.Distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dist.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normaltest(log_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: closer, but still not normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: \n",
    "\n",
    "The square root transformation is another transformation that can transform non-normally distributed data into normally distributed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructor Solution\n",
    "\n",
    "sqrt_dist = np.sqrt(df4.Distance)\n",
    "plt.hist(sqrt_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box cox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The box cox transformation is a parametrized transformation that tries to get distributions \"as close to a normal distribution as possible\".\n",
    "\n",
    "It is defined as:\n",
    "\n",
    "$$ \\text{boxcox}(y_i) = \\frac{y_i^{\\lambda} - 1}{\\lambda} $$\n",
    "\n",
    "You can think of as a generalization of the square root function: the square root function uses the exponent of 0.5, but box cox lets its exponent vary so it can find the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_result = boxcox(df4.Distance)\n",
    "boxcox_dist = bc_result[0]\n",
    "lam = bc_result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['Distance'].hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(boxcox_dist);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "normaltest(boxcox_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significantly more normally distributed (according to p value) than the other two distributions - above 0.05, even!\n",
    "\n",
    "Now that we have a normally distributed y-variable, let's try a regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import (StandardScaler, \n",
    "                                   PolynomialFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = \"Distance\"\n",
    "\n",
    "X = df4.drop(y_col, axis=1)\n",
    "y = df4[y_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Polynomial Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_pf = pf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_pf, y, test_size=0.3, \n",
    "                                                    random_state=72018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit `StandardScaler` on `X_train` as before**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = StandardScaler()\n",
    "X_train_s = s.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_result2 = boxcox(y_train)\n",
    "y_train_bc = bc_result2[0]\n",
    "lam2 = bc_result2[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we'll now:\n",
    "\n",
    "1. Fit regression\n",
    "1. Transform testing data\n",
    "1. Predict on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_bc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train_s, y_train_bc)\n",
    "X_test_s = s.transform(X_test)\n",
    "y_pred_bc = lr.predict(X_test_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "* Are we done?\n",
    "* What did we predict?\n",
    "* How would you interpret these predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inverse transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every transformation has an inverse transformation. The inverse transformation of $f(x) = \\sqrt{x}$ is $f^{-1}(x) = x^2$, for example. Box cox has an inverse transformation as well: notice that we have to pass in the lambda value that we found from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import inv_boxcox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from above\n",
    "bc_result = boxcox(df4.Distance)\n",
    "boxcox_dist = bc_result[0]\n",
    "lam = bc_result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_boxcox(boxcox_dist, lam)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['Distance'].values[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructor Solution\n",
    "y_pred_tran = inv_boxcox(y_pred_bc,lam2)\n",
    "r2_score(y_pred_tran,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB Exercise: \n",
    "\n",
    "### Determine the R^2 of a LinearRegression without the box cox transformation. Is it higher or lower?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_s,y_train)\n",
    "lr_pred = lr.predict(X_test_s)\n",
    "r2_score(lr_pred,y_test)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normaltest(sqrt_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df4.dtypes == np.object\n",
    "categorical_cols = df4.columns[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how many extra columns would be created\n",
    "import numpy as np\n",
    "num_ohc_cols = (df4[categorical_cols]\n",
    "                .apply(lambda x: x.nunique())\n",
    "                .sort_values(ascending=False))\n",
    "\n",
    "\n",
    "# No need to encode if there is only one value\n",
    "small_num_ohc_cols = num_ohc_cols.loc[num_ohc_cols>1]\n",
    "\n",
    "# Number of one-hot columns is one less than the number of categories\n",
    "small_num_ohc_cols -= 1\n",
    "\n",
    "# This is 215 columns, assuming the original ones are dropped. \n",
    "# This is quite a few extra columns!\n",
    "small_num_ohc_cols.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Copy of the data\n",
    "data_ohc = df4.copy()\n",
    "\n",
    "# The encoders\n",
    "le = LabelEncoder()\n",
    "ohc = OneHotEncoder()\n",
    "\n",
    "for col in num_ohc_cols.index:\n",
    "    \n",
    "    # Integer encode the string categories\n",
    "    dat = le.fit_transform(data_ohc[col]).astype(np.int)\n",
    "    \n",
    "    # Remove the original column from the dataframe\n",
    "    data_ohc = data_ohc.drop(col, axis=1)\n",
    "\n",
    "    # One hot encode the data--this returns a sparse array\n",
    "    new_dat = ohc.fit_transform(dat.reshape(-1,1))\n",
    "\n",
    "    # Create unique column names\n",
    "    n_cols = new_dat.shape[1]\n",
    "    col_names = ['_'.join([col, str(x)]) for x in range(n_cols)]\n",
    "\n",
    "    # Create the new dataframe\n",
    "    new_df = pd.DataFrame(new_dat.toarray(), \n",
    "                          index=data_ohc.index, \n",
    "                          columns=col_names)\n",
    "\n",
    "    # Append the new data to the dataframe\n",
    "    data_ohc = pd.concat([data_ohc, new_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column difference is as calculated above\n",
    "data_ohc.shape[1] - df4.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_col = 'Distance'\n",
    "\n",
    "# Split the data that is not one-hot encoded\n",
    "feature_cols = [x for x in df4.columns if x != y_col]\n",
    "X_data = df4[feature_cols]\n",
    "y_data = df4[y_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, \n",
    "                                                    test_size=0.3, random_state=42)\n",
    "# Split the data that is one-hot encoded\n",
    "feature_cols = [x for x in data_ohc.columns if x != y_col]\n",
    "X_data_ohc = data_ohc[feature_cols]\n",
    "y_data_ohc = data_ohc[y_col]\n",
    "\n",
    "X_train_ohc, X_test_ohc, y_train_ohc, y_test_ohc = train_test_split(X_data_ohc, y_data_ohc, \n",
    "                                                    test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the indices to ensure they are identical\n",
    "(X_train_ohc.index == X_train.index).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "LR = LinearRegression()\n",
    "\n",
    "# Storage for error values\n",
    "error_df = list()\n",
    "\n",
    "# Data that have not been one-hot encoded\n",
    "LR = LR.fit(X_train, y_train)\n",
    "y_train_pred = LR.predict(X_train)\n",
    "y_test_pred = LR.predict(X_test)\n",
    "\n",
    "error_df.append(pd.Series({'train': mean_squared_error(y_train, y_train_pred),\n",
    "                           'test' : mean_squared_error(y_test,  y_test_pred)},\n",
    "                           name='no enc'))\n",
    "\n",
    "# Data that have been one-hot encoded\n",
    "LR = LR.fit(X_train_ohc, y_train_ohc)\n",
    "y_train_ohc_pred = LR.predict(X_train_ohc)\n",
    "y_test_ohc_pred = LR.predict(X_test_ohc)\n",
    "\n",
    "error_df.append(pd.Series({'train': mean_squared_error(y_train_ohc, y_train_ohc_pred),\n",
    "                           'test' : mean_squared_error(y_test_ohc,  y_test_ohc_pred)},\n",
    "                          name='one-hot enc'))\n",
    "\n",
    "# Assemble the results\n",
    "error_df = pd.concat(error_df, axis=1)\n",
    "error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mute the setting wtih a copy warnings\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "\n",
    "\n",
    "scalers = {'standard': StandardScaler(),\n",
    "           'minmax': MinMaxScaler(),\n",
    "           'maxabs': MaxAbsScaler()}\n",
    "\n",
    "training_test_sets = {\n",
    "    'not_encoded': (X_train, y_train, X_test, y_test),\n",
    "    'one_hot_encoded': (X_train_ohc, y_train_ohc, X_test_ohc, y_test_ohc)}\n",
    "\n",
    "\n",
    "# Get the list of float columns, and the float data\n",
    "# so that we don't scale something we already scaled. \n",
    "# We're supposed to scale the original data each time\n",
    "mask = X_train.dtypes == np.float\n",
    "float_columns = X_train.columns[mask]\n",
    "\n",
    "# initialize model\n",
    "LR = LinearRegression()\n",
    "\n",
    "# iterate over all possible combinations and get the errors\n",
    "errors = {}\n",
    "for encoding_label, (_X_train, _y_train, _X_test, _y_test) in training_test_sets.items():\n",
    "    for scaler_label, scaler in scalers.items():\n",
    "        trainingset = _X_train.copy()  # copy because we dont want to scale this more than once.\n",
    "        testset = _X_test.copy()\n",
    "        trainingset[float_columns] = scaler.fit_transform(trainingset[float_columns])\n",
    "        testset[float_columns] = scaler.transform(testset[float_columns])\n",
    "        LR.fit(trainingset, _y_train)\n",
    "        predictions = LR.predict(testset)\n",
    "        key = encoding_label + ' - ' + scaler_label + 'scaling'\n",
    "        errors[key] = mean_squared_error(_y_test, predictions)\n",
    "\n",
    "errors = pd.Series(errors)\n",
    "print(errors.to_string())\n",
    "print('-' * 80)\n",
    "for key, error_val in errors.items():\n",
    "    print(key, error_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "sns.set_context('talk')\n",
    "sns.set_style('ticks')\n",
    "sns.set_palette('dark')\n",
    "\n",
    "ax = plt.axes()\n",
    "# we are going to use y_test, y_test_pred\n",
    "ax.scatter(y_test, y_test_pred, alpha=.5)\n",
    "\n",
    "ax.set(xlabel='Distance', \n",
    "       ylabel='Predictions',\n",
    "       title='Predictions vs Distance, using Linear Regression');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Section 2, Part c: Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the shape of the array (rows by a single column)\n",
    "X = df4.drop('Distance', axis=1)\n",
    "y = df4.Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(shuffle=True, random_state=72018, n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"Train index:\", train_index[:10], len(train_index))\n",
    "    print(\"Test index:\",test_index[:10], len(test_index))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = StandardScaler()\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Pipeline([(\"scaler\", s),\n",
    "                      (\"regression\", lr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cross_val_predict(estimator, X, y, cv=kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(scores) # almost identical!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.geomspace(1e-9, 1e0, num=10)\n",
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "coefs = []\n",
    "for alpha in alphas:\n",
    "    las = Lasso(alpha=alpha, max_iter=100000)\n",
    "    \n",
    "    estimator = Pipeline([\n",
    "        (\"scaler\", s),\n",
    "        (\"lasso_regression\", las)])\n",
    "\n",
    "    predictions = cross_val_predict(estimator, X, y, cv = kf)\n",
    "    \n",
    "    score = r2_score(y, predictions)\n",
    "    \n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(alphas,scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso(alpha=1e-6).fit(X, y).coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso(alpha=1.0).fit(X, y).coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.semilogx(alphas, scores, '-o')\n",
    "plt.xlabel('$\\\\alpha$')\n",
    "plt.ylabel('$R^2$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = PolynomialFeatures(degree=3)\n",
    "\n",
    "scores = []\n",
    "alphas = np.geomspace(0.06, 6.0, 20)\n",
    "for alpha in alphas:\n",
    "    las = Lasso(alpha=alpha, max_iter=100000)\n",
    "    \n",
    "    estimator = Pipeline([\n",
    "        (\"scaler\", s),\n",
    "        (\"make_higher_degree\", pf),\n",
    "        (\"lasso_regression\", las)])\n",
    "\n",
    "    predictions = cross_val_predict(estimator, X, y, cv = kf)\n",
    "    \n",
    "    score = r2_score(y, predictions)\n",
    "    \n",
    "    scores.append(score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(alphas, scores);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we have found the hyperparameter (alpha~1e-2=0.01)\n",
    "# make the model and train it on ALL the data\n",
    "# Then release it into the wild .....\n",
    "best_estimator = Pipeline([\n",
    "                    (\"scaler\", s),\n",
    "                    (\"make_higher_degree\", PolynomialFeatures(degree=2)),\n",
    "                    (\"lasso_regression\", Lasso(alpha=0.03))])\n",
    "\n",
    "best_estimator.fit(X, y)\n",
    "best_estimator.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pf = PolynomialFeatures(degree=2)\n",
    "alphas = np.geomspace(4, 20, 20)\n",
    "scores=[]\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha, max_iter=100000)\n",
    "\n",
    "    estimator = Pipeline([\n",
    "        (\"scaler\", s),\n",
    "        (\"polynomial_features\", pf),\n",
    "        (\"ridge_regression\", ridge)])\n",
    "\n",
    "    predictions = cross_val_predict(estimator, X, y, cv = kf)\n",
    "    score = r2_score(y, predictions)\n",
    "    scores.append(score)\n",
    "\n",
    "plt.plot(alphas, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we have found the hyperparameter (alpha~1e-2=0.01)\n",
    "# make the model and train it on ALL the data\n",
    "# Then release it into the wild .....\n",
    "best_estimator = Pipeline([\n",
    "                    (\"scaler\", s),\n",
    "                    (\"make_higher_degree\", PolynomialFeatures(degree=2)),\n",
    "                    (\"lasso_regression\", Lasso(alpha=0.03))])\n",
    "\n",
    "best_estimator.fit(X, y)\n",
    "best_estimator.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importances = pd.DataFrame(zip(best_estimator.named_steps[\"make_higher_degree\"].get_feature_names(),\n",
    "                 best_estimator.named_steps[\"lasso_regression\"].coef_,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names_dict = dict(zip(list(range(len(X.columns.values))), X.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importances.sort_values(by=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Same estimator as before\n",
    "estimator = Pipeline([(\"scaler\", StandardScaler()),\n",
    "        (\"polynomial_features\", PolynomialFeatures()),\n",
    "        (\"ridge_regression\", Ridge())])\n",
    "\n",
    "params = {\n",
    "    'polynomial_features__degree': [1, 2, 3],\n",
    "    'ridge_regression__alpha': np.geomspace(4, 20, 30)\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator, params, cv=kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_score_, grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = grid.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This includes both in-sample and out-of-sample\n",
    "r2_score(y, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that \"grid\" is a fit object!\n",
    "# We can use grid.predict(X_test) to get brand new predictions!\n",
    "grid.best_estimator_.named_steps['ridge_regression'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Setup the polynomial features\n",
    "degree = 20\n",
    "pf = PolynomialFeatures(degree)\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Extract the X- and Y- data from the dataframe \n",
    "X_data = df4[['Distance']]\n",
    "Y_data = df4['Minutes']\n",
    "\n",
    "# Create the features and fit the model\n",
    "X_poly = pf.fit_transform(X_data)\n",
    "lr = lr.fit(X_poly, Y_data)\n",
    "Y_pred = lr.predict(X_poly)\n",
    "\n",
    "# Plot the result\n",
    "plt.plot(X_data, Y_data, marker='o', ls='', label='data', alpha=1)\n",
    "plt.plot(X_real, Y_real, ls='--', label='real function')\n",
    "plt.plot(X_data, Y_pred, marker='^', alpha=.5, label='predictions w/ polynomial features')\n",
    "plt.legend()\n",
    "ax = plt.gca()\n",
    "ax.set(xlabel='x data', ylabel='y data');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mute the sklearn warning about regularization\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module='sklearn')\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "# The ridge regression model\n",
    "rr = Ridge(alpha=0.001)\n",
    "rr = rr.fit(X_poly, Y_data)\n",
    "Y_pred_rr = rr.predict(X_poly)\n",
    "\n",
    "# The lasso regression model\n",
    "lassor = Lasso(alpha=0.0001)\n",
    "lassor = lassor.fit(X_poly, Y_data)\n",
    "Y_pred_lr = lassor.predict(X_poly)\n",
    "\n",
    "# The plot of the predicted values\n",
    "plt.plot(X_data, Y_data, marker='o', ls='', label='data')\n",
    "plt.plot(X_real, Y_real, ls='--', label='real function')\n",
    "plt.plot(X_data, Y_pred, label='linear regression', marker='^', alpha=.5)\n",
    "plt.plot(X_data, Y_pred_rr, label='ridge regression', marker='^', alpha=.5)\n",
    "plt.plot(X_data, Y_pred_lr, label='lasso regression', marker='^', alpha=.5)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set(xlabel='x data', ylabel='y data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the absolute value of coefficients for each model\n",
    "\n",
    "coefficients = pd.DataFrame()\n",
    "coefficients['linear regression'] = lr.coef_.ravel()\n",
    "coefficients['ridge regression'] = rr.coef_.ravel()\n",
    "coefficients['lasso regression'] = lassor.coef_.ravel()\n",
    "coefficients = coefficients.applymap(abs)\n",
    "\n",
    "coefficients.describe()  # Huge difference in scale between non-regularized vs regularized regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette()\n",
    "\n",
    "# Setup the dual y-axes\n",
    "ax1 = plt.axes()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot the linear regression data\n",
    "ax1.plot(lr.coef_.ravel(), \n",
    "         color=colors[0], marker='o', label='linear regression')\n",
    "\n",
    "# Plot the regularization data sets\n",
    "ax2.plot(rr.coef_.ravel(), \n",
    "         color=colors[1], marker='o', label='ridge regression')\n",
    "\n",
    "ax2.plot(lassor.coef_.ravel(), \n",
    "         color=colors[2], marker='o', label='lasso regression')\n",
    "\n",
    "# Customize axes scales\n",
    "ax1.set_ylim(-2e14, 2e14)\n",
    "ax2.set_ylim(-25, 25)\n",
    "\n",
    "# Combine the legends\n",
    "h1, l1 = ax1.get_legend_handles_labels()\n",
    "h2, l2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(h1+h2, l1+l2)\n",
    "\n",
    "ax1.set(xlabel='coefficients',ylabel='linear regression')\n",
    "ax2.set(ylabel='ridge and lasso regression')\n",
    "\n",
    "ax1.set_xticks(range(len(lr.coef_)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def rmse(ytrue, ypredicted):\n",
    "    return np.sqrt(mean_squared_error(ytrue, ypredicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linearRegression = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "linearRegression_rmse = rmse(y_test, linearRegression.predict(X_test))\n",
    "\n",
    "print(linearRegression_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(6,6))\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.plot(y_test, linearRegression.predict(X_test), \n",
    "         marker='o', ls='', ms=3.0)\n",
    "\n",
    "lim = (0, y_test.max())\n",
    "\n",
    "ax.set(xlabel='Actual Distance', \n",
    "       ylabel='Predicted Distance', \n",
    "       xlim=lim,\n",
    "       ylim=lim,\n",
    "       title='Linear Regression Results');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linearRegression = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "linearRegression_rmse = rmse(y_test, linearRegression.predict(X_test))\n",
    "\n",
    "print(linearRegression_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "alphas = [0.005, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 80]\n",
    "\n",
    "ridgeCV = RidgeCV(alphas=alphas, \n",
    "                  cv=4).fit(X_train, y_train)\n",
    "\n",
    "ridgeCV_rmse = rmse(y_test, ridgeCV.predict(X_test))\n",
    "\n",
    "print(ridgeCV.alpha_, ridgeCV_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "alphas2 = np.array([1e-5, 5e-5, 0.0001, 0.0005])\n",
    "\n",
    "lassoCV = LassoCV(alphas=alphas2,\n",
    "                  max_iter=5e4,\n",
    "                  cv=3).fit(X_train, y_train)\n",
    "\n",
    "lassoCV_rmse = rmse(y_test, lassoCV.predict(X_test))\n",
    "\n",
    "print(lassoCV.alpha_, lassoCV_rmse)  # Lasso is slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "l1_ratios = np.linspace(0.1, 0.9, 9)\n",
    "\n",
    "elasticNetCV = ElasticNetCV(alphas=alphas2, \n",
    "                            l1_ratio=l1_ratios,\n",
    "                            max_iter=1e4).fit(X_train, y_train)\n",
    "elasticNetCV_rmse = rmse(y_test, elasticNetCV.predict(X_test))\n",
    "\n",
    "print(elasticNetCV.alpha_, elasticNetCV.l1_ratio_, elasticNetCV_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_vals = [linearRegression_rmse, ridgeCV_rmse, lassoCV_rmse, elasticNetCV_rmse]\n",
    "\n",
    "labels = ['Linear', 'Ridge', 'Lasso', 'ElasticNet']\n",
    "\n",
    "rmse_df = pd.Series(rmse_vals, index=labels).to_frame()\n",
    "rmse_df.rename(columns={0: 'RMSE'}, inplace=1)\n",
    "rmse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(6,6))\n",
    "ax = plt.axes()\n",
    "\n",
    "labels = ['Ridge', 'Lasso', 'ElasticNet']\n",
    "\n",
    "models = [ridgeCV, lassoCV, elasticNetCV]\n",
    "\n",
    "for mod, lab in zip(models, labels):\n",
    "    ax.plot(y_test, mod.predict(X_test), \n",
    "             marker='o', ls='', ms=3.0, label=lab)\n",
    "\n",
    "\n",
    "leg = plt.legend(frameon=True)\n",
    "leg.get_frame().set_edgecolor('black')\n",
    "leg.get_frame().set_linewidth(1.0)\n",
    "\n",
    "ax.set(xlabel='Actual Distance', \n",
    "       ylabel='Predicted Distance', \n",
    "       title='Linear Regression Results');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SGDRegressor and prepare the parameters\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "model_parameters_dict = {\n",
    "    'Linear': {'penalty': 'none'},\n",
    "    'Lasso': {'penalty': 'l2',\n",
    "           'alpha': lassoCV.alpha_},\n",
    "    'Ridge': {'penalty': 'l1',\n",
    "           'alpha': ridgeCV_rmse},\n",
    "    'ElasticNet': {'penalty': 'elasticnet', \n",
    "                   'alpha': elasticNetCV.alpha_,\n",
    "                   'l1_ratio': elasticNetCV.l1_ratio_}\n",
    "}\n",
    "\n",
    "new_rmses = {}\n",
    "for modellabel, parameters in model_parameters_dict.items():\n",
    "    # following notation passes the dict items as arguments\n",
    "    SGD = SGDRegressor(**parameters)\n",
    "    SGD.fit(X_train, y_train)\n",
    "    new_rmses[modellabel] = rmse(y_test, SGD.predict(X_test))\n",
    "\n",
    "rmse_df['RMSE-SGD'] = pd.Series(new_rmses)\n",
    "rmse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SGDRegressor and prepare the parameters\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "model_parameters_dict = {\n",
    "    'Linear': {'penalty': 'none'},\n",
    "    'Lasso': {'penalty': 'l2',\n",
    "           'alpha': lassoCV.alpha_},\n",
    "    'Ridge': {'penalty': 'l1',\n",
    "           'alpha': ridgeCV_rmse},\n",
    "    'ElasticNet': {'penalty': 'elasticnet', \n",
    "                   'alpha': elasticNetCV.alpha_,\n",
    "                   'l1_ratio': elasticNetCV.l1_ratio_}\n",
    "}\n",
    "\n",
    "new_rmses = {}\n",
    "for modellabel, parameters in model_parameters_dict.items():\n",
    "    # following notation passes the dict items as arguments\n",
    "    SGD = SGDRegressor(eta0=1e-7, **parameters)\n",
    "    SGD.fit(X_train, y_train)\n",
    "    new_rmses[modellabel] = rmse(y_test, SGD.predict(X_test))\n",
    "\n",
    "rmse_df['RMSE-SGD-learningrate'] = pd.Series(new_rmses)\n",
    "rmse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "new_rmses = {}\n",
    "for modellabel, parameters in model_parameters_dict.items():\n",
    "    # following notation passes the dict items as arguments\n",
    "    SGD = SGDRegressor(**parameters)\n",
    "    SGD.fit(X_train_scaled, y_train)\n",
    "    new_rmses[modellabel] = rmse(y_test, SGD.predict(X_test_scaled))\n",
    "\n",
    "rmse_df['RMSE-SGD-scaled'] = pd.Series(new_rmses)\n",
    "rmse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "new_rmses = {}\n",
    "for modellabel, parameters in model_parameters_dict.items():\n",
    "    # following notation passes the dict items as arguments\n",
    "    SGD = SGDRegressor(**parameters)\n",
    "    SGD.fit(X_train_scaled, y_train)\n",
    "    new_rmses[modellabel] = rmse(y_test, SGD.predict(X_test_scaled))\n",
    "\n",
    "rmse_df['RMSE-SGD-scaled'] = pd.Series(new_rmses)\n",
    "rmse_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df4[['Distance']], df4['Avg_Pace'], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a linear regression instance\n",
    "reg = LinearRegression(fit_intercept=True)\n",
    "\n",
    "# Train the model on the training set.\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on the testing set and evaluate the performance\n",
    "score = reg.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.set(style='darkgrid')\n",
    "sns.boxplot(x='Distance', y='Avg_Pace', data=df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_variables = pd.get_dummies(df4[['Distance','Avg_Pace','Calories_Burned']], drop_first=True)\n",
    "dummy_variables.shape\n",
    "(2394, 57)\n",
    "dummy_variables.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = df4[['Distance', 'Avg_Pace', 'Workout_Time', 'Calories_Burned']]\n",
    "sc = MinMaxScaler()\n",
    "num_features = sc.fit_transform(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = dummy_variables.values\n",
    "data = np.concatenate((cat_features, num_features), axis=1)\n",
    "X = data[:, :data.shape[1]-1]\n",
    "y = data[:, data.shape[1]-1]\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "(2394, 60)\n",
    "(2394,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "#Create a ridge regressor object\n",
    "ridge = Ridge(alpha=0.5)\n",
    "#Train the model\n",
    "ridge.fit(X_train, y_train)\n",
    "#Evaluate the model\n",
    "print('R-squared score (training):{:.3f}'.\n",
    "format(ridge.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    ".format(ridge.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "#Create a GradientBoostingRegressor object\n",
    "params = {'n_estimators': 600, 'max_depth': 5,\n",
    "'learning_rate': 0.02, 'loss': 'ls'}\n",
    "gbr = GradientBoostingRegressor(**params)\n",
    "#Train the model\n",
    "gbr.fit(X_train, y_train)\n",
    "#Evaluate the model\n",
    "print('R-squared score (training): {:.3f}'\n",
    ".format(gbr.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    ".format(gbr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardizing** data refers to transforming each variable so that it more closely follows a **standard** normal distribution, with mean 0 and standard deviation 1.\n",
    "\n",
    "The [`StandardScaler`](http://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) object in SciKit Learn can do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate X and y**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = \"Distance\"\n",
    "\n",
    "X = df4.drop(y_col, axis=1)\n",
    "y = df4[y_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import, fit, and transform using `StandardScaler`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "s = StandardScaler()\n",
    "X_ss = s.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficients with and without scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "y_col = \"Distance\"\n",
    "\n",
    "X = df4.drop(y_col, axis=1)\n",
    "y = df4[y_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X, y)\n",
    "print(lr.coef_) # min = -18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = StandardScaler()\n",
    "X_ss = s.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = LinearRegression()\n",
    "lr2.fit(X_ss, y)\n",
    "print(lr2.coef_) # coefficients now \"on the same scale\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "# Part 1\n",
    "\n",
    "# Decreasing regularization and ensuring convergence\n",
    "las001 = Lasso(alpha = 0.001, max_iter=100000)\n",
    "\n",
    "# Transforming training set to get standardized units\n",
    "X_train_s = s.fit_transform(X_train)\n",
    "\n",
    "# Fitting model to training set\n",
    "las001.fit(X_train_s, y_train)\n",
    "\n",
    "# Transforming test set using the parameters defined from training set\n",
    "X_test_s = s.transform(X_test)\n",
    "\n",
    "# Finding prediction on test set\n",
    "y_pred = las001.predict(X_test_s)\n",
    "\n",
    "# Calculating r2 score\n",
    "print(\"r2 score for alpha = 0.001:\", r2_score(y_pred, y_test))\n",
    "\n",
    "\n",
    "# Part 2\n",
    "\n",
    "# Using vanilla Linear Regression\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Fitting model to training set\n",
    "lr.fit(X_train_s, y_train)\n",
    "\n",
    "# predicting on test set\n",
    "y_pred_lr = lr.predict(X_test_s)\n",
    "\n",
    "# Calculating r2 score\n",
    "print(\"r2 score for Linear Regression:\", r2_score(y_pred_lr, y_test))\n",
    "\n",
    "\n",
    "# Part 3\n",
    "print('Magnitude of Lasso coefficients:', abs(las001.coef_).sum())\n",
    "print('Number of coeffients not equal to 0 for Lasso:', (las001.coef_!=0).sum())\n",
    "\n",
    "print('Magnitude of Linear Regression coefficients:', abs(lr.coef_).sum())\n",
    "print('Number of coeffients not equal to 0 for Linear Regression:', (lr.coef_!=0).sum())\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 vs. L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the deck: `Lasso` and `Ridge` regression have the same syntax in SciKit Learn.\n",
    "[`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n",
    "Now we're going to compare the results from Ridge vs. Lasso regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# Decreasing regularization and ensuring convergence\n",
    "r = Ridge(alpha = 0.001)\n",
    "X_train_s = s.fit_transform(X_train)\n",
    "r.fit(X_train_s, y_train)\n",
    "X_test_s = s.transform(X_test)\n",
    "y_pred_r = r.predict(X_test_s)\n",
    "\n",
    "# Calculating r2 score\n",
    "r.coef_\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(np.abs(r.coef_)))\n",
    "print(np.sum(np.abs(las001.coef_)))\n",
    "\n",
    "print(np.sum(r.coef_ != 0))\n",
    "print(np.sum(las001.coef_ != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_ss, y, test_size=0.3, \n",
    "                                                    random_state=72018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "r2_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                    random_state=72018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = StandardScaler()\n",
    "lr_s = LinearRegression()\n",
    "X_train_s = s.fit_transform(X_train)\n",
    "lr_s.fit(X_train_s, y_train)\n",
    "X_test_s = s.transform(X_test)\n",
    "y_pred_s = lr_s.predict(X_test_s)\n",
    "r2_score(y_pred_s, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# Calculate the correlation values\n",
    "feature_cols = df4.columns[:-1]\n",
    "corr_values = df4[feature_cols].corr()\n",
    "\n",
    "# Simplify by emptying all the data below the diagonal\n",
    "tril_index = np.tril_indices_from(corr_values)\n",
    "\n",
    "# Make the unused values NaNs\n",
    "for coord in zip(*tril_index):\n",
    "    corr_values.iloc[coord[0], coord[1]] = np.NaN\n",
    "    \n",
    "# Stack the data and convert to a data frame\n",
    "corr_values = (corr_values\n",
    "               .stack()\n",
    "               .to_frame()\n",
    "               .reset_index()\n",
    "               .rename(columns={'level_0':'feature1',\n",
    "                                'level_1':'feature2',\n",
    "                                0:'correlation'}))\n",
    "\n",
    "# Get the absolute values for sorting\n",
    "corr_values['abs_correlation'] = corr_values.correlation.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "sns.set_style('white')\n",
    "\n",
    "ax = corr_values.abs_correlation.hist(bins=50, figsize=(12, 8))\n",
    "ax.set(xlabel='Absolute Correlation', ylabel='Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most highly correlated values\n",
    "corr_values.sort_values('correlation', ascending=False).query('abs_correlation>0.8')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Regression Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df4['Distance'] = le.fit_transform(df4.Distance)\n",
    "df4['Distance'].sample(5)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# Calculate the correlation values\n",
    "feature_cols = df4.columns[:-1]\n",
    "corr_values = df4[feature_cols].corr()\n",
    "\n",
    "# Simplify by emptying all the data below the diagonal\n",
    "tril_index = np.tril_indices_from(corr_values)\n",
    "\n",
    "# Make the unused values NaNs\n",
    "for coord in zip(*tril_index):\n",
    "    corr_values.iloc[coord[0], coord[1]] = np.NaN\n",
    "    \n",
    "# Stack the data and convert to a data frame\n",
    "corr_values = (corr_values\n",
    "               .stack()\n",
    "               .to_frame()\n",
    "               .reset_index()\n",
    "               .rename(columns={'level_0':'feature1',\n",
    "                                'level_1':'feature2',\n",
    "                                0:'correlation'}))\n",
    "\n",
    "# Get the absolute values for sorting\n",
    "corr_values['abs_correlation'] = corr_values.correlation.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "sns.set_style('white')\n",
    "\n",
    "ax = corr_values.abs_correlation.hist(bins=50, figsize=(12, 8))\n",
    "ax.set(xlabel='Absolute Correlation', ylabel='Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most highly correlated values\n",
    "corr_values.sort_values('correlation', ascending=False).query('abs_correlation>0.8')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Regression Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a random state\n",
    "rs = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 20, 30, 50, 100, 150]\n",
    "labels = ['Short', 'Average', 'Long', 'Really Long', 'Too Long']\n",
    "df4['Distances'] = pd.cut(df4['Minutes'], bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdbin = pd.qcut(df4['Minutes'], 3)\n",
    "# bin_labels_10 = (['Short', 'Medium', 'Long'])\n",
    "# df4['Minutes_q'] = pd.qcut(df4['Minutes'], q=[0, .33, .66, 1], labels = bin_labels_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = list(df4.iloc[:, :-1].columns)\n",
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.iloc[:, :-1].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.iloc[:, -1:].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "pd1 = df4.iloc[:, -1:].value_counts().plot.bar()\n",
    "pd1.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = df4.iloc[:, :-1]\n",
    "y_raw = df4.iloc[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scaling the raw input features\n",
    "# X = scaler.fit_transform(X_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The range of feature inputs are within {X.min()} to {X.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LabelEncoder object\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target variable\n",
    "y = label_encoder.fit_transform(y_raw.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's split the training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state = rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training dataset shape, X_train: {X_train.shape}, y_train: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Testing dataset shape, X_test: {X_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 penalty to shrink coefficients without removing any features from the model\n",
    "penalty= 'l2'\n",
    "# Our classification problem is multinomial\n",
    "multi_class = 'multinomial'\n",
    "# Use lbfgs for L2 penalty and multinomial classes\n",
    "solver = 'lbfgs'\n",
    "# Max iteration = 1000\n",
    "max_iter = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a logistic regression model with above arguments\n",
    "l2_model = LogisticRegression(random_state=rs, penalty=penalty, multi_class=multi_class, solver=solver, max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_preds = l2_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(yt, yp):\n",
    "    results_pos = {}\n",
    "    results_pos['accuracy'] = accuracy_score(yt, yp)\n",
    "    precision, recall, f_beta, _ = precision_recall_fscore_support(yt, yp)\n",
    "    results_pos['recall'] = recall\n",
    "    results_pos['precision'] = precision\n",
    "    results_pos['f1score'] = f_beta\n",
    "    return results_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_metrics(y_test, l2_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 penalty to shrink coefficients without removing any features from the model\n",
    "penalty= 'l1'\n",
    "# Our classification problem is multinomial\n",
    "multi_class = 'multinomial'\n",
    "# Use saga for L1 penalty and multinomial classes\n",
    "solver = 'saga'\n",
    "# Max iteration = 1000\n",
    "max_iter = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a logistic regression model with above arguments\n",
    "l1_model = LogisticRegression(random_state=rs, penalty=penalty, multi_class=multi_class, solver=solver, max_iter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_preds = l1_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odd_ratios = l1_model.predict_proba(X_test[:1, :])[0]\n",
    "odd_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_model.predict(X_test[:1, :])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_metrics(y_test, l1_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = confusion_matrix(y_test, l1_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12))\n",
    "ax = sns.heatmap(cf, annot=True) \n",
    "xticklabels=['Short', 'Average', 'Long', 'Really Long', 'Too Long']\n",
    "yticklabels=['Short', 'Average', 'Long', 'Really Long', 'Too Long']\n",
    "ax.set(title=\"Confusion Matrix\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and sort feature coefficients\n",
    "def get_feature_coefs(regression_model, label_index, columns):\n",
    "    coef_dict = {}\n",
    "    for coef, feat in zip(regression_model.coef_[label_index, :], columns):\n",
    "        if abs(coef) >= 0.01:\n",
    "            coef_dict[feat] = coef\n",
    "    # Sort coefficients\n",
    "    coef_dict = {k: v for k, v in sorted(coef_dict.items(), key=lambda item: item[1])}\n",
    "    return coef_dict\n",
    "\n",
    "# Generate bar colors based on if value is negative or positive\n",
    "def get_bar_colors(values):\n",
    "    color_vals = []\n",
    "    for val in values:\n",
    "        if val <= 0:\n",
    "            color_vals.append('r')\n",
    "        else:\n",
    "            color_vals.append('g')\n",
    "    return color_vals\n",
    "\n",
    "# Visualize coefficients\n",
    "def visualize_coefs(coef_dict):\n",
    "    features = list(coef_dict.keys())\n",
    "    values = list(coef_dict.values())\n",
    "    y_pos = np.arange(len(features))\n",
    "    color_vals = get_bar_colors(values)\n",
    "    plt.rcdefaults()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.barh(y_pos, values, align='center', color=color_vals)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(features)\n",
    "    # labels read top-to-bottom\n",
    "    ax.invert_yaxis()  \n",
    "    ax.set_xlabel('Feature Coefficients')\n",
    "    ax.set_title('')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficents for Class 1, Less Often\n",
    "coef_dict = get_feature_coefs(l1_model, 1, feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_coefs(coef_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients for Class 2\n",
    "coef_dict = get_feature_coefs(l1_model, 2, feature_cols)\n",
    "visualize_coefs(coef_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# standard imports and setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "# model evaluation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, ShuffleSplit\n",
    "from sklearn.metrics import *\n",
    "# pipelines\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "# data preparation\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.utils import resample\n",
    "from imblearn.datasets import make_imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# machine learning\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier, XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_num = df4.select_dtypes(include = ['float64', 'int64'])\n",
    "run_num_corr = run_num.corr()['Distance'][:-1]\n",
    "top_features = run_num_corr[abs(run_num_corr) > 0.5].sort_values(ascending=False) #displays pearsons correlation coefficient greater than 0.5\n",
    "print(\"There is {} strongly correlated values with SalePrice:\\n{}\".format(len(top_features), top_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(run_num.columns), 5):\n",
    "    sns.pairplot(data=run_num,\n",
    "                x_vars=run_num.columns[i:i+5],\n",
    "                y_vars=['Distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_untransformed = sns.distplot(df4['Distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skewness: %f\" % df4['Distance'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_transformed = np.log(df4['Distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_transformed = sns.distplot(log_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skewness: %f\" % (log_transformed).skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate = df4[df4.duplicated(['Workout_Time'])]\n",
    "duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHR0 = df4[['Avg_Heart_Rate']] < 0.1\n",
    "AHR = df4[['Avg_Heart_Rate']].mean().round(2)\n",
    "AHR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df4.drop('Distance',axis=1)\n",
    "y = df4.Distance\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts().plot(kind='bar')\n",
    "plt.title('label balance')\n",
    "plt.xlabel('label values')\n",
    "plt.ylabel('amount per label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df4, vars = df4.columns[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df4:\n",
    "    sns.displot(x=column, data=df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert x column to numpy array\n",
    "X = df4.loc[:, ['Calories_Burned']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(X, kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(X, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "#make this example reproducible\n",
    "np.random.seed(0)\n",
    "\n",
    "#create data\n",
    "x = np.random.normal(size=1000)\n",
    "\n",
    "#create normal distribution histogram\n",
    "sns.displot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "#make this example reproducible\n",
    "np.random.seed(0)\n",
    "\n",
    "#create data\n",
    "#X = np.random.normal(size=1000)\n",
    "\n",
    "#create normal distribution curve\n",
    "sns.displot(X, kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "#make this example reproducible\n",
    "np.random.seed(0)\n",
    "\n",
    "#create data\n",
    "#x = np.random.normal(size=1000)\n",
    "\n",
    "#create normal distribution curve\n",
    "sns.displot(df4, x='Minutes', kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data = df4, vars = df.columns[3:9], hue = 'Minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "Running Regression.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/Cbhami/Cosmo/blob/master/DA%20590/Session%2028.ipynb",
     "timestamp": 1648769406675
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "9f357b62676163e453d3996bd7a23625b1faadc1b4915e4cbea894e22b2eb803"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
